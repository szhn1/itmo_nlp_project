{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c924b36-1282-4556-9779-087f2340ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data/cleaned_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f29a6e-586c-4ad7-8b1a-0325f718ad24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers sentence-transformers vllm torch rank_bm25 faiss-cpu accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b01f542-6106-4c50-b883-d5d19fbe5a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-15 01:26:35] INFO loader.py:120: Loading faiss with AVX512 support.\n",
      "[2026-01-15 01:26:35] INFO loader.py:122: Successfully loaded faiss with AVX512 support.\n",
      "[2026-01-15 01:26:43] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: intfloat/multilingual-e5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af947aacba974413918f5e6fbfe9de4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d241566d328640ccb61200ba06127174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdb7400c8da43be9c5245c83aed0660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090b60f5d5ae4d21880b577fbc11ca68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f34a6ca86a43a3a8fc4720a5af43d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f51b2c1c0164699a05d8bb8df7dbb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1678900b3d4447588cfa8707593bc09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625c005af53540a0afecc65688f75ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634679a5d14e4991b1c8b6b8b8836053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d9ac4a54f848039f040daea934c81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c895186aca431ab66e7aea0f6ec578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_docs: <class 'numpy.ndarray'> float32 (123273, 1024) is ndarray: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM\n",
    "from temporal_rag import (\n",
    "    TemporalRAGIndexBuilder,\n",
    "    TemporalRAGPipeline,\n",
    "    tokenize_ru,\n",
    ")\n",
    "\n",
    "# строим индексы для ретривала\n",
    "builder = TemporalRAGIndexBuilder(\n",
    "    save_dir=\"indexes_test\",\n",
    "    encoder_name=\"intfloat/multilingual-e5-large\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "art = builder.build(df, batch_size=64)\n",
    "builder.save(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07aa5c79-1d67-4370-905b-8be107c94bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from vllm import LLM\n",
    "# from temporal_rag import (\n",
    "#     TemporalRAGIndexBuilder,\n",
    "#     TemporalRAGPipeline,\n",
    "#     tokenize_ru,\n",
    "# )\n",
    "\n",
    "# # выгружаем индексы для ретривала\n",
    "# builder = TemporalRAGIndexBuilder(\n",
    "#     save_dir=\"indexes_test\",\n",
    "#     encoder_name=\"intfloat/multilingual-e5-large\",\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# df2, encoder, index, bm25 = builder.load(df, build_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e35dfc4a-3732-4b61-a82d-dbe7a9383274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-15 01:45:13 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 16000, 'gpu_memory_utilization': 0.89, 'max_num_seqs': 32, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-32B-Instruct'}\n",
      "INFO 01-15 01:45:14 [model.py:514] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 01-15 01:45:14 [model.py:1661] Using max model len 16000\n",
      "INFO 01-15 01:45:14 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:45:21 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-32B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-32B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 64, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m WARNING 01-15 01:45:21 [network_utils.py:36] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:45:21 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.200.24.92:34033 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:45:22 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:45:22 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-32B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m /home/mlcore/conda/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:45:25 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:09,  1.61it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:01<00:11,  1.30it/s]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:02<00:12,  1.16it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:03<00:11,  1.15it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:04<00:10,  1.16it/s]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:05<00:09,  1.16it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:05<00:08,  1.16it/s]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:06<00:07,  1.15it/s]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:07<00:06,  1.15it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:08<00:05,  1.22it/s]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:09<00:04,  1.23it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:10<00:04,  1.20it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:10<00:03,  1.18it/s]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:11<00:02,  1.16it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:12<00:01,  1.14it/s]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:13<00:00,  1.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:14<00:00,  1.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:14<00:00,  1.17it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:45:41 [default_loader.py:308] Loading weights took 14.79 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:45:42 [gpu_model_runner.py:3659] Model loading took 61.0375 GiB memory and 18.923066 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:45:53 [backends.py:643] Using cache directory: /home/mlcore/.cache/vllm/torch_compile_cache/956bf58fae/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:45:53 [backends.py:703] Dynamo bytecode transform time: 11.64 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:46:00 [backends.py:261] Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:46:17 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 16.79 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:46:17 [monitor.py:34] torch.compile takes 28.43 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:46:18 [gpu_worker.py:375] Available KV cache memory: 8.01 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:46:19 [kv_cache_utils.py:1291] GPU KV cache size: 32,816 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:46:19 [kv_cache_utils.py:1296] Maximum concurrency for 16,000 tokens per request: 2.05x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.51it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:46:21 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.92 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1343)\u001b[0;0m INFO 01-15 01:46:21 [core.py:259] init engine (profile, create kv cache, warmup model) took 39.32 seconds\n",
      "INFO 01-15 01:46:23 [llm.py:360] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# подгружаем ллм\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM\n",
    "\n",
    "MODEL = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=False)\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL,\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=16000,\n",
    "    gpu_memory_utilization=0.89,\n",
    "    disable_log_stats=True,\n",
    "    max_num_seqs=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3032f9-4bfd-402c-87f5-d5dda5c2b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем пайплайн\n",
    "from system_prompt import SYSTEM_PROMPT\n",
    "\n",
    "pipe = TemporalRAGPipeline(\n",
    "    df=art.df,\n",
    "    index=art.index,\n",
    "    encoder=art.encoder,\n",
    "    bm25=art.bm25,\n",
    "    tokenize_fn=tokenize_ru,\n",
    "    sum_llm=llm,\n",
    "    sum_tokenizer=tokenizer,\n",
    "    judge_llm=llm,\n",
    "    judge_tokenizer=tokenizer,\n",
    "    system_prompt=SYSTEM_PROMPT,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05659017-a43e-4dc1-bea2-0dcc8d4d8e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dae75f7fa445fd92a4972464d8812e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38e3fe356654f78bb6620c5c5cb1544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571939a275cc487b8f3a444c258c7460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb466d078c714edba82671b692fd9a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec34d7191bed45988113f2843abff099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a8b0cde2c84a20abcce2d8fdfff193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449ad1e79cc54d0595f74c1c01ffc01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9bb4c451f24d9baf1a3fc70c3201e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0103d5e7b984467986a8b234f20a1edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bf8ff26bd9463aae35518ff176d103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cb9bf3bf1645a2b388ae9b6ef9cdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92221afec9d4ed9b938648868813d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1) Запрос и актуальная дата\n",
      "* Запрос: Почему кофе дорожает?\n",
      "* Актуальная дата: 2025-09-04\n",
      "\n",
      "### 2) Дайджест\n",
      "Последнее время, с 2025-08-30, кофе сорта робуста и арабика подорожал на мировых рынках, увеличившись в цене в 1,5 раза и на 29% соответственно. Это связано с проблемами с урожаем в Бразилии и Вьетнаме из-за неблагоприятных погодных условий, таких как засуха и проливные дожди. В рамках проекта по борьбе с производством кокаина в Колумбии, фермеров убеждают выращивать кофе вместо коки, что также может повлиять на рынок кофе. Ранее, с 2025-08-01, в России ожидалось новое подорожание кофе на 20-40% из-за проблем с урожаем в Бразилии и Вьетнаме. В целом, тема подорожания кофе остается актуальной и вирусной, поскольку цены продолжают расти из-за неблагоприятных погодных условий и проблем с урожаем.\n",
      "\n",
      "### 3) Таймлайн\n",
      "* 2025-09-01 — Фермеров в Колумбии убеждают выращивать кофе вместо коки\n",
      "* 2025-08-30 — Кофе сорта робуста и арабика подорожал на мировых рынках\n",
      "* 2025-08-26 — Шоколад для россиян станет дороже из-за нехватки качественного сырья\n",
      "* 2025-08-24 — Coca-Cola изучает возможность продажи сети кофеен Costa Coffee\n",
      "* 2025-08-19 — Индекс кофе с бутербродом в России впервые за лето подешевел\n",
      "* 2025-08-01 — В России ожидается новое подорожание кофе на 20-40%\n",
      "* 2025-07-10 — Эксперты прогнозируют подорожание кофе на 40% в этом году\n",
      "* 2025-06-16 — Кофе и какао могут подорожать из-за торговых войн\n",
      "* 2025-05-14 — Торги фьючерсами на кофе на срочном рынке Мосбиржи начнутся с 20 мая\n",
      "* 2025-04-30 — В России растет интерес к альтернативным горячим напиткам из-за дороговизны кофе\n",
      "* 2025-04-19 — Колумбийский эксперт прогнозирует продолжение роста цен на кофе\n",
      "* 2025-04-07 — Кофе в России может подорожать на 50% уже в этом году\n",
      "* 2025-03-24 — Участники рынка предупреждают о возможном резком подорожании кофе на 40%\n",
      "* 2025-03-07 — Мировая торговля кофе практически остановилась из-за роста цен\n",
      "* 2025-03-04 — Кофе рекордно подорожает на 30-40% уже к концу года\n",
      "* 2025-02-09 — Растворимый кофе в магазинах начали продавать за 1,5 тысячи рублей\n",
      "* 2025-01-29 — Биржевые цены на кофе сорта арабика вновь бьют рекорды\n",
      "* 2024-12-23 — Кофе в России крайне сильно подорожает в 2025 году\n",
      "* 2024-12-13 — Цены на кофе продолжают расти и после достижения рекордных значений\n",
      "* 2024-12-02 — Участники рынка прогнозируют дальнейший рост розничной стоимости кофе\n",
      "* 2024-12-01 — Кофе в России станет дороже ещё на 25% из-за засухи в Бразилии и тайфунов во Вьетнаме\n",
      "* 2024-11-27 — Стоимость кофе сорта арабика продолжает расти на фоне опасений за неурожай в Бразилии\n",
      "* 2024-10-31 — Продажи кофе на вынос в России за год увеличились на четверть\n",
      "* 2024-10-24 — Продажи кофе на вынос выросли на 25% за год\n",
      "* 2024-10-01 — Цены на зерновой кофе удвоились и продолжат расти\n",
      "* 2024-09-26 — Кофе в России резко подорожает до 30% к концу года\n",
      "* 2024-09-16 — Цены на кофе взлетели до максимума за 13 лет\n"
     ]
    }
   ],
   "source": [
    "# ответ\n",
    "out = pipe.answer(\n",
    "    query=\"Почему кофе дорожает?\",\n",
    "    anchor_date=\"2025-09-04\", # дата обзора\n",
    "    k_retrieve=150, # глубина ретривала\n",
    "    topN_each=2000, # количество документов для каждого метода ретривала (бм25 / семантика)\n",
    "    k_docs=30, # количество документов в суммаризации\n",
    "    snip_chars=1000, # размер каждой новости\n",
    "    max_new_tokens=5000, # ограничение ответа\n",
    "    max_window_days=365, # максимальный промежуток времени ретривала\n",
    "    w_time=0.5, # вес даты для ретривала\n",
    "    hot_window_days=30, # окно для определения актуальной новости\n",
    "    hot_ratio=0.7, # доля актуальных новостей в ответе из топ-к\n",
    ")\n",
    "\n",
    "print(out.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1583b18-46c7-46d2-8c0a-e071ebed8a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "local/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
