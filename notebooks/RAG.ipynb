{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d488cb-8c54-4177-8d80-d3bb6ef323e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: openpyxl in /home/mlcore/conda/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: pandas in /home/mlcore/conda/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/mlcore/conda/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: et-xmlfile in /home/mlcore/conda/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mlcore/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mlcore/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mlcore/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/mlcore/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install openpyxl pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f399f7-a595-4b07-b733-bb6367941289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/conda/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "channels = pd.read_excel(\"../data/tg_channels.xlsx\")\n",
    "#df = pd.read_csv(\"../data/cleaned_news_exp.csv\")[[\"message_id\", \"id_channel\", \"message\", \"date\", \"topic\"]]\n",
    "df = pd.read_parquet(\"../data/tg_news_full.parquet\")[[\"message_id\", \"id_channel\", \"message\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf2a27c-2557-4aef-acc6-bfdf9bea8481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>id_channel</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>channel_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275548</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –°–ª–æ–≤–∞–∫–∏–∏ –æ–±—Å—É–¥–∏—Ç –º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º...</td>\n",
       "      <td>2025-01-02 17:00:02</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275547</td>\n",
       "      <td>3</td>\n",
       "      <td>–í –î–¢–ü —Å —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∞–≤—Ç–æ–±—É—Å–æ–º –≤ –¢–∞–∏–ª–∞–Ω–¥–µ –ø–æ—Å...</td>\n",
       "      <td>2025-01-02 16:40:53</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>275546</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü—Ä–µ–º—å–µ—Ä –ò–∑—Ä–∞–∏–ª—è –ù–µ—Ç–∞–Ω—å—è—Ö—É –≤—ã–ø–∏—Å–∞–Ω –∏–∑ –±–æ–ª—å–Ω–∏—Ü—ã ...</td>\n",
       "      <td>2025-01-02 16:20:12</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>275545</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –≤ –ø–æ–¥—Ä—ã–≤–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è Tesla Cyber...</td>\n",
       "      <td>2025-01-02 15:54:29</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275543</td>\n",
       "      <td>3</td>\n",
       "      <td>–°–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è. –û–±—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ ...</td>\n",
       "      <td>2025-01-02 15:32:55</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   message_id  id_channel                                            message  \\\n",
       "0      275548           3  –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –°–ª–æ–≤–∞–∫–∏–∏ –æ–±—Å—É–¥–∏—Ç –º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º...   \n",
       "1      275547           3  –í –î–¢–ü —Å —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∞–≤—Ç–æ–±—É—Å–æ–º –≤ –¢–∞–∏–ª–∞–Ω–¥–µ –ø–æ—Å...   \n",
       "2      275546           3  –ü—Ä–µ–º—å–µ—Ä –ò–∑—Ä–∞–∏–ª—è –ù–µ—Ç–∞–Ω—å—è—Ö—É –≤—ã–ø–∏—Å–∞–Ω –∏–∑ –±–æ–ª—å–Ω–∏—Ü—ã ...   \n",
       "3      275545           3  –ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –≤ –ø–æ–¥—Ä—ã–≤–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è Tesla Cyber...   \n",
       "4      275543           3  –°–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è. –û–±—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ ...   \n",
       "\n",
       "                 date channel_name  \n",
       "0 2025-01-02 17:00:02  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  \n",
       "1 2025-01-02 16:40:53  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  \n",
       "2 2025-01-02 16:20:12  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  \n",
       "3 2025-01-02 15:54:29  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  \n",
       "4 2025-01-02 15:32:55  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_map = (channels[[\"id\", \"name\"]]\n",
    "          .dropna()\n",
    "          .assign(id=lambda x: pd.to_numeric(x[\"id\"], errors=\"coerce\"))\n",
    "          .dropna(subset=[\"id\"])\n",
    "          .assign(id=lambda x: x[\"id\"].astype(int))\n",
    "          .set_index(\"id\")[\"name\"]\n",
    "          .to_dict())\n",
    "\n",
    "df = df.copy()\n",
    "df[\"id_channel\"] = pd.to_numeric(df[\"id_channel\"], errors=\"coerce\")\n",
    "df[\"channel_name\"] = df[\"id_channel\"].map(ch_map).fillna(df[\"id_channel\"].astype(\"Int64\").astype(str))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d52ac2-bde7-4917-87f2-c3f26a844a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_news_text(t: str) -> str:\n",
    "    t = t or \"\"\n",
    "    t = re.sub(r\"#\\w+\", \" \", t)\n",
    "    t = re.sub(r\"[‚ö°Ô∏èüìàüìâüá∑üá∫‚úÖ‚ùóÔ∏èüî•‚¨õ ‚¨ú ‚ö´ ‚ö™üîπ]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, col: str = \"date\") -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[col] = pd.to_datetime(d[col], utc=True, errors=\"coerce\")\n",
    "    d = d.dropna(subset=[col])\n",
    "    d[\"date_day\"] = d[col].dt.floor(\"D\")\n",
    "    return d\n",
    "\n",
    "df = ensure_datetime(df, \"date\")\n",
    "df[\"message_id\"] = df[\"message_id\"].astype(str)\n",
    "df[\"message\"] = df[\"message\"].fillna(\"\").astype(str).map(clean_news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b05d5ab-1cbd-4a6d-8f62-825f02d0456b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>id_channel</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>date_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275548</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –°–ª–æ–≤–∞–∫–∏–∏ –æ–±—Å—É–¥–∏—Ç –º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º...</td>\n",
       "      <td>2025-01-02 17:00:02+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275547</td>\n",
       "      <td>3</td>\n",
       "      <td>–í –î–¢–ü —Å —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∞–≤—Ç–æ–±—É—Å–æ–º –≤ –¢–∞–∏–ª–∞–Ω–¥–µ –ø–æ—Å...</td>\n",
       "      <td>2025-01-02 16:40:53+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>275546</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü—Ä–µ–º—å–µ—Ä –ò–∑—Ä–∞–∏–ª—è –ù–µ—Ç–∞–Ω—å—è—Ö—É –≤—ã–ø–∏—Å–∞–Ω –∏–∑ –±–æ–ª—å–Ω–∏—Ü—ã ...</td>\n",
       "      <td>2025-01-02 16:20:12+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>275545</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –≤ –ø–æ–¥—Ä—ã–≤–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è Tesla Cyber...</td>\n",
       "      <td>2025-01-02 15:54:29+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275543</td>\n",
       "      <td>3</td>\n",
       "      <td>–°–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è. –û–±—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ ...</td>\n",
       "      <td>2025-01-02 15:32:55+00:00</td>\n",
       "      <td>–†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  message_id  id_channel                                            message  \\\n",
       "0     275548           3  –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –°–ª–æ–≤–∞–∫–∏–∏ –æ–±—Å—É–¥–∏—Ç –º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º...   \n",
       "1     275547           3  –í –î–¢–ü —Å —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∞–≤—Ç–æ–±—É—Å–æ–º –≤ –¢–∞–∏–ª–∞–Ω–¥–µ –ø–æ—Å...   \n",
       "2     275546           3  –ü—Ä–µ–º—å–µ—Ä –ò–∑—Ä–∞–∏–ª—è –ù–µ—Ç–∞–Ω—å—è—Ö—É –≤—ã–ø–∏—Å–∞–Ω –∏–∑ –±–æ–ª—å–Ω–∏—Ü—ã ...   \n",
       "3     275545           3  –ü–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã–π –≤ –ø–æ–¥—Ä—ã–≤–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è Tesla Cyber...   \n",
       "4     275543           3  –°–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è. –û–±—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ ...   \n",
       "\n",
       "                       date channel_name                  date_day  \n",
       "0 2025-01-02 17:00:02+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  \n",
       "1 2025-01-02 16:40:53+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  \n",
       "2 2025-01-02 16:20:12+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  \n",
       "3 2025-01-02 15:54:29+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  \n",
       "4 2025-01-02 15:32:55+00:00  –†–∏–∞ –ù–æ–≤–æ—Å—Ç–∏ 2025-01-02 00:00:00+00:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c82283b6-cbae-483e-8495-f4170eed9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77a872d-7698-435d-a369-479b7b1e534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def tokenize_ru(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^0-9a-z–∞-—è—ë\\s]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()\n",
    "\n",
    "corpus_tok = [tokenize_ru(t) for t in df[\"message\"].tolist()]\n",
    "bm25 = BM25Okapi(corpus_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ea3567-02fb-40d0-a13e-e77de60fe2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install faiss-cpu faiss-gpu-cu12 faiss-gpu-cu11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4290f08a-82d8-40b6-af73-e89d0386bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-large\" \n",
    "encoder = SentenceTransformer(MODEL_NAME, device=\"cuda\")  \n",
    "texts = df[\"message\"].tolist()\n",
    "doc_inputs = [\"passage: \" + t for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7436ed1c-6f09-413b-ae3b-03d76af07d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E_docs = encoder.encode(\n",
    "#     doc_inputs,\n",
    "#     batch_size=64,\n",
    "#     show_progress_bar=True,\n",
    "#     normalize_embeddings=True,\n",
    "# ).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb37ea91-457f-407e-bff0-7e743bf4430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "\n",
    "# dim = E_docs.shape[1]\n",
    "# index = faiss.IndexFlatIP(dim)      \n",
    "# index.add(E_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab301b0c-3979-48dd-97f9-c3c16f713b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import faiss\n",
    "# import pickle\n",
    "\n",
    "# OUT = Path(\"indexes\")\n",
    "# OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# #rowmap\n",
    "# rowmap = df[[\"message_id\",\"date\",\"date_day\",\"id_channel\",\"channel_name\"]].copy()\n",
    "# rowmap.to_parquet(OUT / \"rowmap.parquet\", index=False)\n",
    "\n",
    "# np.save(OUT / \"E_docs_e5_large.npy\", E_docs)\n",
    "\n",
    "# faiss.write_index(index, str(OUT / \"faiss_e5_large.index\"))\n",
    "\n",
    "# with open(OUT / \"bm25_corpus_tok.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(corpus_tok, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c4f45-33ca-49b9-b735-599e5d851055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af2760a8-4f82-4159-899f-e94447a94e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "INP = Path(\"indexes\")\n",
    "\n",
    "rowmap = pd.read_parquet(INP / \"rowmap.parquet\")\n",
    "\n",
    "E_docs = np.load(INP / \"E_docs_e5_large.npy\")\n",
    "index = faiss.read_index(str(INP / \"faiss_e5_large.index\"))\n",
    "\n",
    "with open(INP / \"bm25_corpus_tok.pkl\", \"rb\") as f:\n",
    "    corpus_tok = pickle.load(f)\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "bm25 = BM25Okapi(corpus_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcef3d8f-f6d5-4ded-b27d-b86e1b53b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(rowmap) == E_docs.shape[0]\n",
    "assert len(rowmap) == index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17969e2a-8bfc-414a-86a7-da15a18cfedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde85e14-f892-47a2-b585-4af1c1b8b5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f76acc5-fc79-4909-bfdd-c4211312fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def snippet(t: str, n: int = 1000) -> str:\n",
    "    return t[:n]\n",
    "\n",
    "def _topk_indices_from_scores(scores: np.ndarray, k: int) -> np.ndarray:\n",
    "    k = min(k, len(scores))\n",
    "    if k <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if k == len(scores):\n",
    "        idx = np.argsort(-scores)\n",
    "    else:\n",
    "        idx = np.argpartition(-scores, k-1)[:k]\n",
    "        idx = idx[np.argsort(-scores[idx])]\n",
    "    return idx.astype(int)\n",
    "\n",
    "def dense_candidates_faiss(index, encoder, query: str, topN: int = 500):\n",
    "    qv = encoder.encode([\"query: \" + query], normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
    "    scores, idx = index.search(qv, topN) \n",
    "    return idx[0].astype(int), scores[0].astype(np.float32)\n",
    "\n",
    "\n",
    "def hybrid_retrieve_rrf(\n",
    "    df: pd.DataFrame,\n",
    "    index,\n",
    "    encoder,\n",
    "    bm25,\n",
    "    tokenize_fn,\n",
    "    query: str,\n",
    "    k: int = 50,\n",
    "    topN_each: int = 500,\n",
    "    k_rrf: int = 60,\n",
    "    w_dense: float = 1.0,\n",
    "    w_bm25: float = 1.0,\n",
    "    exclude_message_id: str | None = None,\n",
    "    anchor_date: str | pd.Timestamp | None = None,\n",
    "    date_col: str = \"date_day\") -> pd.DataFrame:\n",
    "\n",
    "    if anchor_date is not None:\n",
    "        ad = pd.to_datetime(anchor_date, utc=True).normalize()\n",
    "\n",
    "        if date_col not in df.columns:\n",
    "            raise KeyError(f\"date_col='{date_col}' not found in df.columns\")\n",
    "\n",
    "        dts = pd.to_datetime(df[date_col], errors=\"coerce\", utc=True).dt.normalize()\n",
    "        allowed = (dts <= ad)\n",
    "        allowed_np = allowed.to_numpy(dtype=bool)\n",
    "    else:\n",
    "        allowed_np = None\n",
    "\n",
    "    d_idx, d_sc = dense_candidates_faiss(index, encoder, query, topN=topN_each)\n",
    "\n",
    "    if allowed_np is not None:\n",
    "        keep = allowed_np[d_idx]\n",
    "        d_idx = d_idx[keep]\n",
    "        d_sc = d_sc[keep]\n",
    "\n",
    "    dense_rank = {int(rowpos): r for r, rowpos in enumerate(d_idx, start=1)}\n",
    "\n",
    "    if bm25 is None:\n",
    "        out = df.iloc[d_idx].copy()\n",
    "        out[\"_rowpos\"] = d_idx\n",
    "        out[\"score_rrf\"] = (w_dense / (k_rrf + out[\"_rowpos\"].map(lambda rp: dense_rank.get(int(rp), np.nan)).astype(np.float32)))\n",
    "        out[\"rank_dense\"] = out[\"_rowpos\"].map(lambda rp: dense_rank.get(int(rp), np.nan))\n",
    "        out[\"rank_bm25\"] = np.nan\n",
    "\n",
    "        if exclude_message_id is not None and \"message_id\" in out.columns:\n",
    "            out = out[out[\"message_id\"].astype(str) != str(exclude_message_id)]\n",
    "\n",
    "        return out.head(k).reset_index(drop=True)\n",
    "\n",
    "    bm_scores = bm25.get_scores(tokenize_fn(query)).astype(np.float32)\n",
    "    if allowed_np is not None:\n",
    "        bm_scores[~allowed_np] = -np.inf\n",
    "\n",
    "    b_idx = _topk_indices_from_scores(bm_scores, topN_each)\n",
    "    bm_rank = {int(rowpos): r for r, rowpos in enumerate(b_idx, start=1)}\n",
    "\n",
    "    union = np.array(sorted(set(dense_rank) | set(bm_rank)), dtype=int)\n",
    "    rrf = np.zeros(len(union), dtype=np.float32)\n",
    "\n",
    "    for j, rowpos in enumerate(union):\n",
    "        if rowpos in dense_rank:\n",
    "            rrf[j] += w_dense / (k_rrf + dense_rank[rowpos])\n",
    "        if rowpos in bm_rank:\n",
    "            rrf[j] += w_bm25 / (k_rrf + bm_rank[rowpos])\n",
    "\n",
    "    order = np.argsort(-rrf)\n",
    "    union = union[order]\n",
    "    rrf = rrf[order]\n",
    "\n",
    "    out = df.iloc[union].copy()\n",
    "    out[\"_rowpos\"] = union\n",
    "    out[\"score_rrf\"] = rrf\n",
    "    out[\"rank_dense\"] = out[\"_rowpos\"].map(lambda rp: dense_rank.get(int(rp), np.nan))\n",
    "    out[\"rank_bm25\"] = out[\"_rowpos\"].map(lambda rp: bm_rank.get(int(rp), np.nan))\n",
    "\n",
    "    if exclude_message_id is not None and \"message_id\" in out.columns:\n",
    "        out = out[out[\"message_id\"].astype(str) != str(exclude_message_id)]\n",
    "\n",
    "    return out.head(k).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149ac8fd-e146-4487-8cbb-3731554be0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-09-23 00:00:00+0000', tz='UTC'),\n",
       " Timestamp('2025-09-08 00:00:00+0000', tz='UTC'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"date_day\"].min(), df[\"date_day\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc92482-2a6e-4757-a3a7-6b9a96e0c03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b39a2-ec1c-4e6b-8b3d-8878c8c8a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0464504d-eaa9-480f-bb2f-a62c2e868ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers torch sentence-transformers accelerate vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20fd5266-3f63-4e49-ae0b-795767667a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# MODEL = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"cuda\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91710dd3-0196-4c7b-812c-b319832c1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "VRAM(GB): 79.3\n",
      "Model max_position_embeddings: 32768\n",
      "INFO 01-09 04:47:12 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 24576, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-32B-Instruct'}\n",
      "INFO 01-09 04:47:13 [model.py:514] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 01-09 04:47:13 [model.py:1661] Using max model len 24576\n",
      "INFO 01-09 04:47:13 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:47:20 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-32B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=24576, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-32B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m WARNING 01-09 04:47:20 [network_utils.py:36] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:47:21 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.200.22.124:54875 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:47:21 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:47:21 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-32B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m /home/mlcore/conda/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:47:25 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:10,  1.59it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:01<00:10,  1.43it/s]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:02<00:09,  1.43it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:02<00:09,  1.44it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:03<00:08,  1.46it/s]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:04<00:07,  1.48it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:04<00:07,  1.41it/s]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:05<00:05,  1.53it/s]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:06<00:05,  1.55it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:06<00:04,  1.51it/s]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:07<00:04,  1.46it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:08<00:03,  1.41it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:08<00:02,  1.42it/s]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:09<00:02,  1.45it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:10<00:01,  1.43it/s]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:11<00:00,  1.43it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:11<00:00,  1.45it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:11<00:00,  1.46it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:47:37 [default_loader.py:308] Loading weights took 11.77 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:47:38 [gpu_model_runner.py:3659] Model loading took 61.0375 GiB memory and 15.784764 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:47:50 [backends.py:643] Using cache directory: /home/mlcore/.cache/vllm/torch_compile_cache/956b349265/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:47:50 [backends.py:703] Dynamo bytecode transform time: 11.61 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:48:10 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 13.591 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:48:10 [monitor.py:34] torch.compile takes 25.20 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:48:11 [gpu_worker.py:375] Available KV cache memory: 8.81 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:48:11 [kv_cache_utils.py:1291] GPU KV cache size: 36,064 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:48:11 [kv_cache_utils.py:1296] Maximum concurrency for 24,576 tokens per request: 1.47x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:06<00:00,  7.99it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:03<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:48:21 [gpu_model_runner.py:4587] Graph capturing finished in 10 secs, took 4.11 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=11153)\u001b[0;0m INFO 01-09 04:48:22 [core.py:259] init engine (profile, create kv cache, warmup model) took 43.63 seconds\n",
      "INFO 01-09 04:48:23 [llm.py:360] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "MODEL = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "#MODEL = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(MODEL, trust_remote_code=True)\n",
    "native_ctx = getattr(cfg, \"max_position_embeddings\", None)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"VRAM(GB):\", round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 1))\n",
    "print(\"Model max_position_embeddings:\", native_ctx)\n",
    "\n",
    "MAX_MODEL_LEN = 24576\n",
    "\n",
    "\n",
    "model = LLM(\n",
    "    model=MODEL,\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=0.90,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaab1396-2f2f-4691-80b6-4ddf925d8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"–¢—ã –¥–µ–ª–∞–µ—à—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç –ø–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º.\n",
    "\n",
    "–í—Ö–æ–¥: –∑–∞–ø—Ä–æ—Å, –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD) –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤–∏–¥–∞:\n",
    "date=YYYY-MM-DD channel=<–Ω–∞–∑–≤–∞–Ω–∏–µ>\n",
    "<—Ç–µ–∫—Å—Ç>\n",
    "\n",
    "–ü—Ä–∞–≤–∏–ª–∞:\n",
    "- –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.\n",
    "- –û–ø–∏—Ä–∞–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–Ω–µ –¥–æ–±–∞–≤–ª—è–π –≤–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç—ã).\n",
    "- –ù–µ —É–ø–æ–º–∏–Ω–∞–π –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.\n",
    "- –ï—Å–ª–∏ —Ä—è–¥–æ–º —Å –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–∞—Ç–æ–π –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –º–∞–ª–æ/–Ω–µ—Ç, –ø—Ä—è–º–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –∏ –Ω–µ –¥–µ–ª–∞–π –≤—ã–≤–æ–¥–æ–≤ ‚Äú—á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–µ–π—á–∞—Å‚Äù.\n",
    "\n",
    "–°—Ñ–æ—Ä–º–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ 4 –±–ª–æ–∫–∞—Ö:\n",
    "\n",
    "### 1) –ó–∞–ø—Ä–æ—Å –∏ –¥–∞—Ç–∞\n",
    "* –ó–∞–ø—Ä–æ—Å: <...>\n",
    "* –ê–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞: <...>\n",
    "\n",
    "### 2) –î–∞–π–¥–∂–µ—Å—Ç\n",
    "–°–≤—è–∑–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏: —Å–Ω–∞—á–∞–ª–∞ —á—Ç–æ –±—ã–ª–æ –±–ª–∏–∂–µ –≤—Å–µ–≥–æ –∫ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–∞—Ç–µ, –∑–∞—Ç–µ–º –ø—Ä–µ–¥—ã—Å—Ç–æ—Ä–∏—è/–≤–æ–ª–Ω—ã (–µ—Å–ª–∏ –≤–∏–¥–Ω—ã).\n",
    "–ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–º–µ—á–∞–π —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø–æ–¥–∞—á–µ –∫–∞–Ω–∞–ª–æ–≤.\n",
    "\n",
    "### 3) –í—ã–≤–æ–¥—ã –∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å\n",
    "2‚Äì6 –±—É–ª–ª–µ—Ç–æ–≤: —Å–≤–µ–∂–µ—Å—Ç—å (–ø–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –∏ —Ä–∞–∑—Ä—ã–≤), –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (–≤–æ–ª–Ω–∞/–µ–¥–∏–Ω–∏—á–Ω—ã–µ), —à–∏—Ä–∏–Ω–∞ (—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–∞–ª–æ–≤), –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è (–¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç –ª–∏ –æ–¥–∏–Ω), –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "### 4) –¢–∞–π–º–ª–∞–π–Ω\n",
    "–ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–∞—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ—Ç –Ω–æ–≤–æ–≥–æ –∫ —Å—Ç–∞—Ä–æ–º—É –≤ —Å—Ç–æ–ª–±–∏–∫. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - —É–Ω–∏–∫–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞, –Ω–∞ –∫–æ—Ç–æ—Ä—É—é –µ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏:\n",
    "* YYYY-MM-DD ‚Äî –ö–∞–Ω–∞–ª1; –ö–∞–Ω–∞–ª2: –∫—Ä–∞—Ç–∫–æ —á—Ç–æ —Å–æ–æ–±—â–∞–µ—Ç—Å—è\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def build_rag_context(query: str, cand: pd.DataFrame, anchor_date: str, k_docs: int = 30, snip_chars: int = 850) -> str:\n",
    "    c = cand.head(k_docs).copy()\n",
    "\n",
    "    blocks = []\n",
    "    for i, row in enumerate(c.itertuples(index=False), start=1):\n",
    "        date_day = getattr(row, \"date_day\", getattr(row, \"date\", \"\"))\n",
    "        if isinstance(date_day, pd.Timestamp):\n",
    "            date_day = date_day.strftime(\"%Y-%m-%d\")\n",
    "        date_day = str(date_day)[:10]\n",
    "\n",
    "        channel = getattr(row, \"channel_name\")\n",
    "        text = getattr(row, \"message\", \"\")\n",
    "\n",
    "        blocks.append(f\"[{i}] date={date_day} channel={channel}\\n\" + snippet(str(text), snip_chars))\n",
    "\n",
    "    return (\n",
    "        f\"anchor_date: {anchor_date}\\n\"\n",
    "        f\"–í–û–ü–†–û–°/–ó–ê–ü–†–û–°:\\n{query}\\n\\n\"\n",
    "        f\"–ò–°–¢–û–ß–ù–ò–ö–ò:\\n\" + \"\\n\\n\".join(blocks)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b04c688-b520-422b-ab98-275c68d10ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# @torch.inference_mode()\n",
    "# def rag_summarize(sum_model, sum_tokenizer, query: str, cand: pd.DataFrame, anchor_date, \n",
    "#                   k_docs: int = 25, snip_chars: int = 900, max_new_tokens: int = 2000) -> str:\n",
    "    \n",
    "#     user = build_rag_context(query, cand, anchor_date=anchor_date, k_docs=k_docs, snip_chars=snip_chars)\n",
    "#     print(\"built context...\")\n",
    "    \n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "#         {\"role\": \"user\", \"content\": user},\n",
    "#     ]\n",
    "#     prompt = sum_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     enc = sum_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(sum_model.device)\n",
    "\n",
    "#     out_ids = sum_model.generate(\n",
    "#         **enc,\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         do_sample=False,\n",
    "#         eos_token_id=sum_tokenizer.eos_token_id,\n",
    "#         pad_token_id=sum_tokenizer.eos_token_id,\n",
    "#     )\n",
    "#     prompt_len = int(enc[\"attention_mask\"][0].sum().item())\n",
    "    \n",
    "#     return sum_tokenizer.decode(out_ids[0][prompt_len:], skip_special_tokens=True).strip(), user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79a6f6c0-27c0-48ec-b32f-94dcc09ce244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "def rag_summarize(sum_model, sum_tokenizer, query: str, cand: pd.DataFrame, anchor_date,\n",
    "                  k_docs: int = 25, snip_chars: int = 900, max_new_tokens: int = 2000):\n",
    "\n",
    "    user = build_rag_context(query, cand, anchor_date=anchor_date, k_docs=k_docs, snip_chars=snip_chars)\n",
    "    print(\"built context...\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "    prompt = sum_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    #Mistral\n",
    "    #prompt = f\"<s>[SYSTEM_PROMPT]{SYSTEM_PROMPT}[/SYSTEM_PROMPT][INST]{user}[/INST]\"\n",
    "\n",
    "    sampling = SamplingParams(\n",
    "        temperature=0.0,  \n",
    "        max_tokens=max_new_tokens,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "    result = sum_model.generate([prompt], sampling)[0]\n",
    "    text = result.outputs[0].text.strip()\n",
    "\n",
    "    return text, user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "382a7922-4c1b-4d3f-bbd7-95152e4842e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_hybrid(\n",
    "    df: pd.DataFrame,\n",
    "    index,\n",
    "    encoder,\n",
    "    bm25,\n",
    "    tokenize_fn,\n",
    "    query: str,\n",
    "    exclude_message_id: str | None = None,\n",
    "    k_retrieve: int = 50,\n",
    "    topN_each: int = 500,\n",
    "    k_docs: int = 25,\n",
    "    snip_chars: int = 1500,\n",
    "    max_new_tokens: int = 2000, \n",
    "    anchor_date: str = \"2025-09-04\", \n",
    "\n",
    "    sum_model=None,\n",
    "    sum_tokenizer=None):\n",
    "\n",
    "    \n",
    "    cand = hybrid_retrieve_rrf(\n",
    "        df=df,\n",
    "        index=index,\n",
    "        encoder=encoder,\n",
    "        bm25=bm25,\n",
    "        tokenize_fn=tokenize_fn,\n",
    "        query=query,\n",
    "        k=k_retrieve,\n",
    "        topN_each=topN_each,\n",
    "        k_rrf=60,\n",
    "        w_dense=1.0,\n",
    "        w_bm25=1.0,\n",
    "        exclude_message_id=exclude_message_id,\n",
    "        anchor_date=anchor_date)\n",
    "    \n",
    "    if sum_model is None or sum_tokenizer is None:\n",
    "        ctx = build_rag_context(query, cand, anchor_date=anchor_date, k_docs=k_docs, snip_chars=snip_chars)\n",
    "        return {\"context\": ctx, \"candidates\": cand, \"summary\": \"No LLM\"}\n",
    "    \n",
    "    summary, ctx = rag_summarize(sum_model, sum_tokenizer, query, cand, k_docs=k_docs, snip_chars=snip_chars, max_new_tokens=max_new_tokens, anchor_date=anchor_date)\n",
    "    return {\"context\": ctx, \"summary\": summary, \"candidates\": cand}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453b4b7-8475-4e75-87bf-3fd23950401c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4cdc31c-ad49-4c99-bab8-8d16d73c7f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built context...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e874a669d74d5ebf3588205bdd7d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba108b5dcc79490f97cfb985fab9e00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = \"–ê–∫—Ç—É–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞\"\n",
    "\n",
    "out = run_rag_hybrid(\n",
    "    df=df,\n",
    "    index=index,\n",
    "    encoder=encoder,\n",
    "    bm25=bm25,\n",
    "    tokenize_fn=tokenize_ru,\n",
    "    query=q,\n",
    "    k_retrieve=50,\n",
    "    topN_each=2000,\n",
    "    k_docs=50,\n",
    "    snip_chars=1500,\n",
    "    max_new_tokens = 5000,\n",
    "    anchor_date = \"2025-02-06\",\n",
    "    sum_model=model,           \n",
    "    sum_tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8f8574b-ad55-46f5-b75f-9c8ef1738f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 1) –ó–∞–ø—Ä–æ—Å –∏ –¥–∞—Ç–∞\n",
       "* –ó–∞–ø—Ä–æ—Å: –ê–∫—Ç—É–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞\n",
       "* –ê–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞: 2025-02-06\n",
       "\n",
       "### 2) –î–∞–π–¥–∂–µ—Å—Ç\n",
       "–ù–∞ 3 —Ñ–µ–≤—Ä–∞–ª—è 2025 –≥–æ–¥–∞ –¶–ë –ø–æ–¥–Ω—è–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 2 —Ä—É–±–ª—è, –ø—Ä–∏–±–ª–∏–∑–∏–≤ –µ–≥–æ –∫ 100 —Ä—É–±–ª—è–º. –í –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–Ω–∏ –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –∫–æ–ª–µ–±–∞–ª—Å—è, –Ω–æ –≤ —Ü–µ–ª–æ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª —Ç–µ–Ω–¥–µ–Ω—Ü–∏—é –∫ —Ä–æ—Å—Ç—É. –ù–∞–ø—Ä–∏–º–µ—Ä, 23 —è–Ω–≤–∞—Ä—è –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±—ã–ª —Å–Ω–∏–∂—ë–Ω –¥–æ 98,2804 —Ä—É–±–ª—è, –∞ 21 —è–Ω–≤–∞—Ä—è ‚Äî –¥–æ 101,9579 —Ä—É–±–ª—è. –í –∫–æ–Ω—Ü–µ –¥–µ–∫–∞–±—Ä—è 2024 –≥–æ–¥–∞ –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —Ç–∞–∫–∂–µ —Å–Ω–∏–∂–∞–ª—Å—è, –¥–æ—Å—Ç–∏–≥–Ω—É–≤ 99,22 —Ä—É–±–ª—è –∑–∞ –¥–æ–ª–ª–∞—Ä. –í —Å–µ—Ä–µ–¥–∏–Ω–µ –¥–µ–∫–∞–±—Ä—è –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±—ã–ª –ø–æ–≤—ã—à–µ–Ω –¥–æ 103,3 —Ä—É–±–ª—è, –∞ –∑–∞—Ç–µ–º —Å–Ω–æ–≤–∞ –ø–æ–Ω–∏–∂–µ–Ω –¥–æ 99,3759 —Ä—É–±–ª—è. –í –Ω–æ—è–±—Ä–µ –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ—Å—Ç–∏–≥–∞–ª 114 —Ä—É–±–ª–µ–π, –Ω–æ –∑–∞—Ç–µ–º –Ω–∞—á–∞–ª —Å–Ω–∏–∂–∞—Ç—å—Å—è. –í –Ω–∞—á–∞–ª–µ –¥–µ–∫–∞–±—Ä—è –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±—ã–ª —Å–Ω–∏–∂—ë–Ω –¥–æ 99 —Ä—É–±–ª–µ–π. –í —Å–µ—Ä–µ–¥–∏–Ω–µ –Ω–æ—è–±—Ä—è –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ—Å—Ç–∏–≥–∞–ª 111 —Ä—É–±–ª–µ–π, –∞ –∑–∞—Ç–µ–º –ø—Ä–æ–¥–æ–ª–∂–∞–ª —Ä–∞—Å—Ç–∏ –¥–æ 114 —Ä—É–±–ª–µ–π. –í –Ω–∞—á–∞–ª–µ –Ω–æ—è–±—Ä—è –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±—ã–ª –ø–æ–≤—ã—à–µ–Ω –¥–æ 102 —Ä—É–±–ª–µ–π. –í –æ–∫—Ç—è–±—Ä–µ –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –∫–æ–ª–µ–±–∞–ª—Å—è –æ–∫–æ–ª–æ 96 —Ä—É–±–ª–µ–π. –í –∏—é–Ω–µ –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –±—ã–ª –æ–∫–æ–ª–æ 85 —Ä—É–±–ª–µ–π.\n",
       "\n",
       "### 3) –í—ã–≤–æ–¥—ã –∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å\n",
       "- **–°–≤–µ–∂–µ—Å—Ç—å**: –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ ‚Äî 3 —Ñ–µ–≤—Ä–∞–ª—è 2025 –≥–æ–¥–∞, —Ä–∞–∑—Ä—ã–≤ –¥–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–∞—Ç—ã —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 3 –¥–Ω—è.\n",
       "- **–ü–ª–æ—Ç–Ω–æ—Å—Ç—å**: –ù–æ–≤–æ—Å—Ç–∏ –æ –∫—É—Ä—Å–µ –¥–æ–ª–ª–∞—Ä–∞ –ø–æ—Å—Ç—É–ø–∞–ª–∏ —Ä–µ–≥—É–ª—è—Ä–Ω–æ, –Ω–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–Ω–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–Ω–∏–∑–∏–ª–∞—Å—å.\n",
       "- **–®–∏—Ä–∏–Ω–∞**: –ù–æ–≤–æ—Å—Ç–∏ –æ –∫—É—Ä—Å–µ –¥–æ–ª–ª–∞—Ä–∞ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏—Å—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–Ω–∞–ª–∞—Ö, –≤–∫–ª—é—á–∞—è –ë–ª—É–º–±–µ—Ä–≥, –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞, –°–∏–≥–Ω–∞–ª—ã –†–¶–ë –∏ –¥—Ä—É–≥–∏–µ.\n",
       "- **–ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è**: –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ –∫—É—Ä—Å–µ –¥–æ–ª–ª–∞—Ä–∞ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏—Å—å –Ω–∞ –∫–∞–Ω–∞–ª–∞—Ö –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞ –∏ –°–∏–≥–Ω–∞–ª—ã –†–¶–ë.\n",
       "- **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö**: –ù–æ–≤–æ—Å—Ç–∏ –æ –∫—É—Ä—Å–µ –¥–æ–ª–ª–∞—Ä–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–Ω–∏ –±—ã–ª–∏ –Ω–µ–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏.\n",
       "\n",
       "### 4) –¢–∞–π–º–ª–∞–π–Ω\n",
       "* 2025-02-03 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ—Ç –¶–ë –≤—ã—Ä–æ—Å –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 2 —Ä—É–±–ª—è, –≤–ø–ª–æ—Ç–Ω—É—é –ø—Ä–∏–±–ª–∏–∑–∏–≤—à–∏—Å—å –∫ 100.\n",
       "* 2025-02-03 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –¶–ë –ø–æ–¥–Ω—è–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —Å—Ä–∞–∑—É –Ω–∞ –¥–≤–∞ —Ä—É–±–ª—è, –ø–æ—á—Ç–∏ –¥–æ 100 —Ä—É–±–ª–µ–π.\n",
       "* 2025-01-29 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –¶–ë –†–§ –ø–æ–≤—ã—Å–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ 4,68 –∫–æ–ø., –¥–æ 98,0126 —Ä—É–±.\n",
       "* 2025-01-24 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –¶–ë –†–§ —Å–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ 83,42 –∫–æ–ø., –¥–æ 98,2636 —Ä—É–±.\n",
       "* 2025-01-22 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –¶–ë –†–§ —Å–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ 1,65 —Ä—É–±., –¥–æ 98,2804 —Ä—É–±.\n",
       "* 2025-01-20 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –¶–ë –†–§ —Å–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ 45,58 –∫–æ–ø., –¥–æ 101,9579 —Ä—É–±.\n",
       "* 2024-12-31 ‚Äî –†–∞–Ω—å—à–µ –≤—Å–µ—Ö. –ù—É –ø–æ—á—Ç–∏: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ –≤–Ω–µ–±–∏—Ä–∂–µ–≤—ã—Ö —Ç–æ—Ä–≥–∞—Ö –ø–æ—á—Ç–∏ –ø—Ä–µ–≤—ã—Å–∏–ª 114 —Ä—É–±–ª–µ–π.\n",
       "* 2024-12-26 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –†—É–±–ª—å –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —É–∫—Ä–µ–ø–ª—è—Ç—å—Å—è ‚Äî –¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –≤ 99,22 —Ä—É–±–ª—è –∑–∞ –¥–æ–ª–ª–∞—Ä.\n",
       "* 2024-12-24 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –¶–ë –†–§ —Å–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ 1,74 —Ä—É–±., –¥–æ 99,8729 —Ä—É–±.\n",
       "* 2024-12-24 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–ø–∞–ª –Ω–∏–∂–µ 100.\n",
       "* 2024-12-23 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –¶–ë –†–§ —Å–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ 72,95 –∫–æ–ø., –¥–æ 101,6143 —Ä—É–±.\n",
       "* 2024-12-20 ‚Äî –†–∞–Ω—å—à–µ –≤—Å–µ—Ö. –ù—É –ø–æ—á—Ç–∏: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 102,3438 —Ä—É–±./$1.\n",
       "* 2024-12-14 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ ‚Äî 100-110 —Ä—É–±–ª–µ–π.\n",
       "* 2024-12-11 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –¶–ë –ø–æ–¥–Ω—è–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –≤—ã—à–µ 103 —Ä—É–±–ª–µ–π, –µ–≤—Ä–æ ‚Äî –≤—ã—à–µ 108,5 —Ä—É–±–ª—è.\n",
       "* 2024-12-10 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –¶–ë —Å–Ω–æ–≤–∞ –ø–æ–¥–Ω—è–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –≤—ã—à–µ 100 —Ä—É–±–ª–µ–π, –µ–≤—Ä–æ ‚Äî –≤—ã—à–µ 106.\n",
       "* 2024-12-09 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –¶–ë –†–§ —Å–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ 4,56 –∫–æ–ø., –¥–æ 99,3759 —Ä—É–±.\n",
       "* 2024-12-06 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –¶–ë –ø–æ–Ω–∏–∑–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∏–∂–µ 100, –µ–≤—Ä–æ ‚Äî –Ω–∏–∂–µ 107.\n",
       "* 2024-12-06 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –¶–ë –ø–æ–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 99 —Ä—É–±–ª–µ–π.\n",
       "* 2024-12-05 ‚Äî –†–∞–Ω—å—à–µ –≤—Å–µ—Ö. –ù—É –ø–æ—á—Ç–∏: –ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏ —É—Å—Ç–∞–Ω–æ–≤–∏–ª –Ω–∞ 6 –¥–µ–∫–∞–±—Ä—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ 103,3837 —Ä—É–±.\n",
       "* 2024-12-05 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –Ω–æ–≤—ã–µ –∫—É—Ä—Å—ã –¥–ª—è –¥–æ–ª–ª–∞—Ä–∞ –∏ –µ–≤—Ä–æ ‚Äî 103,3 –∏ 109,7 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.\n",
       "* 2024-12-03 ‚Äî –†–∞–Ω—å—à–µ –≤—Å–µ—Ö. –ù—É –ø–æ—á—Ç–∏: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 106,1878 —Ä—É–±./$1.\n",
       "* 2024-11-28 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ—Ç –¶–ë –ø—Ä–µ–≤—ã—Å–∏–ª 109 —Ä—É–±–ª–µ–π.\n",
       "* 2024-11-28 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –æ–ø—É—Å—Ç–∏–ª—Å—è –Ω–∏–∂–µ –æ—Ç–º–µ—Ç–∫–∏ 110 —Ä—É–±–ª–µ–π.\n",
       "* 2024-11-27 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –î–æ–ª–ª–∞—Ä ‚Äî 114, –µ–≤—Ä–æ ‚Äî 120.\n",
       "* 2024-11-27 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–∂–µ –≤—ã—à–µ 110 —Ä—É–±–ª–µ–π –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ä–∞—Å—Ç–∏.\n",
       "* 2024-11-27 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –ø–µ—Ä–µ–≤–∞–ª–∏–ª –∑–∞ 111 —Ä—É–±–ª–µ–π. UPD: –ö—É—Ä—Å —É–∂–µ 114.\n",
       "* 2024-11-27 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –¶–ë —Å–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ 1,84 —Ä—É–±., –¥–æ 107,7409 —Ä—É–±.\n",
       "* 2024-11-26 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –¶–ë –ø–æ–¥–Ω—è–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 105 —Ä—É–±–ª–µ–π, –µ–≤—Ä–æ ‚Äî –¥–æ 110 —Ä—É–±–ª–µ–π.\n",
       "* 2024-11-25 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ—Å—Ç–∏–≥ –ø–æ—á—Ç–∏ 104 —Ä—É–±–ª–µ–π –ø–æ –æ—Ü–µ–Ω–∫–µ –¶–ë.\n",
       "* 2024-11-22 ‚Äî Forbes Russia: –¶–ë —É—Å—Ç–∞–Ω–æ–≤–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –≤—ã—à–µ 102 —Ä—É–±–ª–µ–π –≤–ø–µ—Ä–≤—ã–µ —Å –º–∞—Ä—Ç–∞ 2022 –≥–æ–¥–∞.\n",
       "* 2024-11-22 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –¶–ë —Ä–µ–∑–∫–æ –ø–æ–≤—ã—Å–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 102 —Ä—É–±–ª–µ–π, –µ–≤—Ä–æ ‚Äî –¥–æ 107 —Ä—É–±–ª–µ–π.\n",
       "* 2024-11-19 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –ø—Ä–µ–≤—ã—Å–∏–ª 100 —Ä—É–±–ª–µ–π.\n",
       "* 2024-11-15 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –¶–ë –ø–æ–¥–Ω—è–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –ø–æ—á—Ç–∏ –¥–æ 100 —Ä—É–±–ª–µ–π.\n",
       "* 2024-11-14 ‚Äî –≠–∫–æ–Ω–æ–º–∏–∫–∞: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —Ç–µ–ø–µ—Ä—å —Å—Ç–æ–∏—Ç –≤—ã—à–µ 100 —Ä—É–±–ª–µ–π.\n",
       "* 2024-11-09 ‚Äî –ë–∞–Ω–∫–∏, –¥–µ–Ω—å–≥–∏, –¥–≤–∞ –æ—Ñ—à–æ—Ä–∞: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ—Å—Ç–∏–≥ 100 —Ä—É–±–ª–µ–π –≤ –º–æ–º–µ–Ω—Ç–µ –Ω–∞ Forex.\n",
       "* 2024-10-11 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –¶–ë –†–§ —Å–Ω–∏–∑–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ 1,17 —Ä—É–±., –¥–æ 96,0686 —Ä—É–±.\n",
       "* 2024-10-07 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ –º–∞–∫—Å–∏–º—É–º–µ —Å 20 –æ–∫—Ç—è–±—Ä—è 2023 –≥–æ–¥–∞.\n",
       "* 2024-10-01 ‚Äî –≠–∫–æ–Ω–æ–º–∏–∫–∞: –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫ –†–æ—Å—Å–∏–∏ —É—Å—Ç–∞–Ω–æ–≤–∏–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ 93,2221 —Ä—É–±–ª—è.\n",
       "* 2024-10-01 ‚Äî –°–∏–≥–Ω–∞–ª—ã –†–¶–ë: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ –±–∏—Ä–∂–µ ICE –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–æ—á—Ç–∏ 96 —Ä—É–±.\n",
       "* 2024-06-27 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –¶–ë –†–§ –ø–æ–Ω–∏–∑–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 84,96 —Ä—É–±., –µ–≤—Ä–æ ‚Äî –¥–æ 90,99 —Ä—É–±., —é–∞–Ω—è ‚Äî –¥–æ 11,48 —Ä—É–±.\n",
       "* 2024-06-20 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –¶–ë –ø–æ–≤—ã—Å–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –¥–æ 85,42 —Ä—É–±–ª—è, –µ–≤—Ä–æ ‚Äî –¥–æ 91,45 —Ä—É–±–ª—è, —é–∞–Ω—è ‚Äî –¥–æ 11,52 —Ä—É–±–ª—è.\n",
       "* 2024-06-13 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –¶–ë –ø–æ—Å–ª–µ —Å–∞–Ω–∫—Ü–∏–π –°–®–ê –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –ú–æ—Å–±–∏—Ä–∂–∏ —É—Å—Ç–∞–Ω–æ–≤–∏–ª –∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ 88,21 —Ä—É–±–ª—è.\n",
       "* 2024-06-13 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–ø–∞–ª –¥–æ 87‚ÇΩ.\n",
       "* 2024-06-12 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –≤ –∫—Ä–∏–ø—Ç–æ–æ–±–º–µ–Ω–Ω–∏–∫–∞—Ö –ø–æ–¥–Ω—è–ª—Å—è –¥–æ 96 —Ä—É–±–ª–µ–π.\n",
       "* 2024-04-30 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ —É–ø–∞–ª –Ω–∏–∂–µ 93‚ÇΩ.\n",
       "* 2024-04-26 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫—É—Ä—Å—ã –≤–∞–ª—é—Ç: –¥–æ–ª–ª–∞—Ä ‚Äì 87.87‚ÇΩ, –µ–≤—Ä–æ ‚Äì 94.1‚ÇΩ.\n",
       "* 2024-04-19 ‚Äî –ë–ª—É–º–±–µ—Ä–≥: –ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å 150-160 —Ä—É–±–ª–µ–π."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "import re\n",
    "\n",
    "def show_summary(summary: str):\n",
    "    if summary is None:\n",
    "        display(HTML(\"<b>summary is None</b>\"))\n",
    "        return\n",
    "\n",
    "    s = str(summary)\n",
    "    s = s.replace(\"\\\\n\", \"\\n\") \n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s).strip()\n",
    "\n",
    "    display(Markdown(s))\n",
    "\n",
    "show_summary(out[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b2f10-82de-46fe-be7d-a187c8d62254",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e25602-c922-4041-abc4-2fd0f98f6c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19deabb-38f1-46f7-817d-ad259a0a74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "props = torch.cuda.get_device_properties(0)\n",
    "vram_gb = props.total_memory / (1024**3)\n",
    "print(\"GPU:\", props.name)\n",
    "print(\"VRAM (GB):\", round(vram_gb, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef50f0-d15a-4e45-a5da-2f723be7dc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae4e7d-5f88-4561-afcb-e55ab536c30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bb82f-7f53-42f3-b16c-05f05dc42352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d1d99-c8a5-4c41-8e12-70ccef824d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ebafb-e7ca-423b-949b-86bad48b8378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58069a8-6326-4db4-a4ad-d93f352638de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db71e0-e65e-4243-bbc9-40a57ee5b077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa86a20-14cf-48d4-89e4-80f66537337e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e1578-a774-4a1f-ba07-c3e2db786e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6a85c-9116-434c-84d9-1bf8fc18ebdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c52345-c60e-4c2e-bd0d-fab4e1d0e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "–î–∞–≤–∞–π –ø–æ–ø—Ä–æ–±—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –º–∞–ª–µ–Ω—å–∫–æ–π –º–æ–¥–µ–ª—å–∫–æ–π JUDGE_MODEL –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ç–æ–ø-–∫ –∏ –ø–æ–º–µ–Ω—è—Ç—å –ø—Ä–æ–º–ø—Ç —Ç–∞–∫, —á—Ç–æ–±—ã, –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –º—ã —è–≤–Ω–æ —Å–∫–∞–∂–µ–º, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± —ç—Ç–æ–º –Ω–µ—Ç, –Ω–æ –≤–æ—Ç —á—Ç–æ –º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å –æ–± —ç—Ç–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ (–ª–ª–º —Å–∫–∞–∂–µ—Ç –æ—Ç —Å–µ–±—è)\n",
    "\n",
    "–Ω—É –∏ –±—É–¥–µ–º –¥–µ–ª–∞—Ç—å –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–π —Ä–µ—Ç—Ä–∏–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34296b50-956e-4686-bf34-1e18d2893733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "local/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
