{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d488cb-8c54-4177-8d80-d3bb6ef323e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: openpyxl in /home/mlcore/conda/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /home/mlcore/conda/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0af36c7-2004-47ce-87df-32e640813227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: pip in /home/mlcore/.local/lib/python3.10/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/pip/25.3/pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/mlcore/conda/lib/python3.10/site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in /home/mlcore/conda/lib/python3.10/site-packages (0.45.1)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-25.2\n",
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Collecting numpy\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/numpy/2.2.6/numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m279.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/pandas/2.3.3/pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m368.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/python-dateutil/2.9.0.post0/python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/pytz/2025.2/pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/tzdata/2025.3/tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/six/1.17.0/six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "\u001b[2K  Attempting uninstall: pytz\n",
      "\u001b[2K    Found existing installation: pytz 2025.2\n",
      "\u001b[2K    Uninstalling pytz-2025.2:\n",
      "\u001b[2K      Successfully uninstalled pytz-2025.2\n",
      "\u001b[2K  Attempting uninstall: tzdataâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/6\u001b[0m [pytz]\n",
      "\u001b[2K    Found existing installation: tzdata 2025.20m \u001b[32m0/6\u001b[0m [pytz]\n",
      "\u001b[2K    Uninstalling tzdata-2025.2:â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/6\u001b[0m [pytz]\n",
      "\u001b[2K      Successfully uninstalled tzdata-2025.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [tzdata]\n",
      "\u001b[2K  Attempting uninstall: six0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [tzdata]\n",
      "\u001b[2K    Found existing installation: six 1.17.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [tzdata]\n",
      "\u001b[2K    Uninstalling six-1.17.0:mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1/6\u001b[0m [tzdata]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/6\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: numpymâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/6\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/6\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/6\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0â”â”\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0â”â”â”â”\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: pandasâ”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/6\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Found existing installation: pandas 2.2.0[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/6\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Uninstalling pandas-2.2.0:\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/6\u001b[0m [python-dateutil]\n",
      "\u001b[2K      Successfully uninstalled pandas-2.2.00m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[32m5/6\u001b[0m [pandas]util]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/6\u001b[0m [pandas]2m5/6\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fsclient 1.1.2 requires numpy<2.0.0, but you have numpy 2.2.6 which is incompatible.\n",
      "fsclient 1.1.2 requires pandas<=2.2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6 pandas-2.3.3 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.3\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install -U pip setuptools wheel\n",
    "! python -m pip install --force-reinstall --no-cache-dir numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f399f7-a595-4b07-b733-bb6367941289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/conda/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "channels = pd.read_excel(\"../data/tg_channels.xlsx\")\n",
    "#df = pd.read_csv(\"../data/cleaned_news_exp.csv\")[[\"message_id\", \"id_channel\", \"message\", \"date\", \"topic\"]]\n",
    "df = pd.read_parquet(\"../data/tg_news_full.parquet\")[[\"message_id\", \"id_channel\", \"message\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf2a27c-2557-4aef-acc6-bfdf9bea8481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>id_channel</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>channel_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275548</td>\n",
       "      <td>3</td>\n",
       "      <td>ĞŸÑ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¡Ğ»Ğ¾Ğ²Ğ°ĞºĞ¸Ğ¸ Ğ¾Ğ±ÑÑƒĞ´Ğ¸Ñ‚ Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼...</td>\n",
       "      <td>2025-01-02 17:00:02</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275547</td>\n",
       "      <td>3</td>\n",
       "      <td>Ğ’ Ğ”Ğ¢ĞŸ Ñ Ñ‚ÑƒÑ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ±ÑƒÑĞ¾Ğ¼ Ğ² Ğ¢Ğ°Ğ¸Ğ»Ğ°Ğ½Ğ´Ğµ Ğ¿Ğ¾Ñ...</td>\n",
       "      <td>2025-01-02 16:40:53</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>275546</td>\n",
       "      <td>3</td>\n",
       "      <td>ĞŸÑ€ĞµĞ¼ÑŒĞµÑ€ Ğ˜Ğ·Ñ€Ğ°Ğ¸Ğ»Ñ ĞĞµÑ‚Ğ°Ğ½ÑŒÑÑ…Ñƒ Ğ²Ñ‹Ğ¿Ğ¸ÑĞ°Ğ½ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ†Ñ‹ ...</td>\n",
       "      <td>2025-01-02 16:20:12</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>275545</td>\n",
       "      <td>3</td>\n",
       "      <td>ĞŸĞ¾Ğ´Ğ¾Ğ·Ñ€ĞµĞ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ² Ğ¿Ğ¾Ğ´Ñ€Ñ‹Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»Ñ Tesla Cyber...</td>\n",
       "      <td>2025-01-02 15:54:29</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275543</td>\n",
       "      <td>3</td>\n",
       "      <td>Ğ¡Ğ¿ĞµÑ†Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞĞ±ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ° ...</td>\n",
       "      <td>2025-01-02 15:32:55</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   message_id  id_channel                                            message  \\\n",
       "0      275548           3  ĞŸÑ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¡Ğ»Ğ¾Ğ²Ğ°ĞºĞ¸Ğ¸ Ğ¾Ğ±ÑÑƒĞ´Ğ¸Ñ‚ Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼...   \n",
       "1      275547           3  Ğ’ Ğ”Ğ¢ĞŸ Ñ Ñ‚ÑƒÑ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ±ÑƒÑĞ¾Ğ¼ Ğ² Ğ¢Ğ°Ğ¸Ğ»Ğ°Ğ½Ğ´Ğµ Ğ¿Ğ¾Ñ...   \n",
       "2      275546           3  ĞŸÑ€ĞµĞ¼ÑŒĞµÑ€ Ğ˜Ğ·Ñ€Ğ°Ğ¸Ğ»Ñ ĞĞµÑ‚Ğ°Ğ½ÑŒÑÑ…Ñƒ Ğ²Ñ‹Ğ¿Ğ¸ÑĞ°Ğ½ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ†Ñ‹ ...   \n",
       "3      275545           3  ĞŸĞ¾Ğ´Ğ¾Ğ·Ñ€ĞµĞ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ² Ğ¿Ğ¾Ğ´Ñ€Ñ‹Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»Ñ Tesla Cyber...   \n",
       "4      275543           3  Ğ¡Ğ¿ĞµÑ†Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞĞ±ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ° ...   \n",
       "\n",
       "                 date channel_name  \n",
       "0 2025-01-02 17:00:02  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸  \n",
       "1 2025-01-02 16:40:53  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸  \n",
       "2 2025-01-02 16:20:12  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸  \n",
       "3 2025-01-02 15:54:29  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸  \n",
       "4 2025-01-02 15:32:55  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_map = (channels[[\"id\", \"name\"]]\n",
    "          .dropna()\n",
    "          .assign(id=lambda x: pd.to_numeric(x[\"id\"], errors=\"coerce\"))\n",
    "          .dropna(subset=[\"id\"])\n",
    "          .assign(id=lambda x: x[\"id\"].astype(int))\n",
    "          .set_index(\"id\")[\"name\"]\n",
    "          .to_dict())\n",
    "\n",
    "df = df.copy()\n",
    "df[\"id_channel\"] = pd.to_numeric(df[\"id_channel\"], errors=\"coerce\")\n",
    "df[\"channel_name\"] = df[\"id_channel\"].map(ch_map).fillna(df[\"id_channel\"].astype(\"Int64\").astype(str))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d52ac2-bde7-4917-87f2-c3f26a844a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_news_text(t: str) -> str:\n",
    "    t = t or \"\"\n",
    "    t = re.sub(r\"#\\w+\", \" \", t)\n",
    "    t = re.sub(r\"[âš¡ï¸ğŸ“ˆğŸ“‰ğŸ‡·ğŸ‡ºâœ…â—ï¸ğŸ”¥â¬› â¬œ âš« âšªğŸ”¹]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, col: str = \"date\") -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[col] = pd.to_datetime(d[col], utc=True, errors=\"coerce\")\n",
    "    d = d.dropna(subset=[col])\n",
    "    d[\"date_day\"] = d[col].dt.floor(\"D\")\n",
    "    return d\n",
    "\n",
    "df = ensure_datetime(df, \"date\")\n",
    "df[\"message_id\"] = df[\"message_id\"].astype(str)\n",
    "df[\"message\"] = df[\"message\"].fillna(\"\").astype(str).map(clean_news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b05d5ab-1cbd-4a6d-8f62-825f02d0456b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>id_channel</th>\n",
       "      <th>message</th>\n",
       "      <th>date</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>date_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275548</td>\n",
       "      <td>3</td>\n",
       "      <td>ĞŸÑ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¡Ğ»Ğ¾Ğ²Ğ°ĞºĞ¸Ğ¸ Ğ¾Ğ±ÑÑƒĞ´Ğ¸Ñ‚ Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼...</td>\n",
       "      <td>2025-01-02 17:00:02+00:00</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275547</td>\n",
       "      <td>3</td>\n",
       "      <td>Ğ’ Ğ”Ğ¢ĞŸ Ñ Ñ‚ÑƒÑ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ±ÑƒÑĞ¾Ğ¼ Ğ² Ğ¢Ğ°Ğ¸Ğ»Ğ°Ğ½Ğ´Ğµ Ğ¿Ğ¾Ñ...</td>\n",
       "      <td>2025-01-02 16:40:53+00:00</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>275546</td>\n",
       "      <td>3</td>\n",
       "      <td>ĞŸÑ€ĞµĞ¼ÑŒĞµÑ€ Ğ˜Ğ·Ñ€Ğ°Ğ¸Ğ»Ñ ĞĞµÑ‚Ğ°Ğ½ÑŒÑÑ…Ñƒ Ğ²Ñ‹Ğ¿Ğ¸ÑĞ°Ğ½ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ†Ñ‹ ...</td>\n",
       "      <td>2025-01-02 16:20:12+00:00</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>275545</td>\n",
       "      <td>3</td>\n",
       "      <td>ĞŸĞ¾Ğ´Ğ¾Ğ·Ñ€ĞµĞ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ² Ğ¿Ğ¾Ğ´Ñ€Ñ‹Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»Ñ Tesla Cyber...</td>\n",
       "      <td>2025-01-02 15:54:29+00:00</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275543</td>\n",
       "      <td>3</td>\n",
       "      <td>Ğ¡Ğ¿ĞµÑ†Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞĞ±ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ° ...</td>\n",
       "      <td>2025-01-02 15:32:55+00:00</td>\n",
       "      <td>Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸</td>\n",
       "      <td>2025-01-02 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  message_id  id_channel                                            message  \\\n",
       "0     275548           3  ĞŸÑ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¡Ğ»Ğ¾Ğ²Ğ°ĞºĞ¸Ğ¸ Ğ¾Ğ±ÑÑƒĞ´Ğ¸Ñ‚ Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼...   \n",
       "1     275547           3  Ğ’ Ğ”Ğ¢ĞŸ Ñ Ñ‚ÑƒÑ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ±ÑƒÑĞ¾Ğ¼ Ğ² Ğ¢Ğ°Ğ¸Ğ»Ğ°Ğ½Ğ´Ğµ Ğ¿Ğ¾Ñ...   \n",
       "2     275546           3  ĞŸÑ€ĞµĞ¼ÑŒĞµÑ€ Ğ˜Ğ·Ñ€Ğ°Ğ¸Ğ»Ñ ĞĞµÑ‚Ğ°Ğ½ÑŒÑÑ…Ñƒ Ğ²Ñ‹Ğ¿Ğ¸ÑĞ°Ğ½ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ†Ñ‹ ...   \n",
       "3     275545           3  ĞŸĞ¾Ğ´Ğ¾Ğ·Ñ€ĞµĞ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ² Ğ¿Ğ¾Ğ´Ñ€Ñ‹Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»Ñ Tesla Cyber...   \n",
       "4     275543           3  Ğ¡Ğ¿ĞµÑ†Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞĞ±ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ° ...   \n",
       "\n",
       "                       date channel_name                  date_day  \n",
       "0 2025-01-02 17:00:02+00:00  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ 2025-01-02 00:00:00+00:00  \n",
       "1 2025-01-02 16:40:53+00:00  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ 2025-01-02 00:00:00+00:00  \n",
       "2 2025-01-02 16:20:12+00:00  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ 2025-01-02 00:00:00+00:00  \n",
       "3 2025-01-02 15:54:29+00:00  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ 2025-01-02 00:00:00+00:00  \n",
       "4 2025-01-02 15:32:55+00:00  Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ 2025-01-02 00:00:00+00:00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82283b6-cbae-483e-8495-f4170eed9fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: rank_bm25 in /home/mlcore/conda/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /home/mlcore/conda/lib/python3.10/site-packages (from rank_bm25) (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77a872d-7698-435d-a369-479b7b1e534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def tokenize_ru(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^0-9a-zĞ°-ÑÑ‘\\s]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()\n",
    "\n",
    "corpus_tok = [tokenize_ru(t) for t in df[\"message\"].tolist()]\n",
    "bm25 = BM25Okapi(corpus_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ea3567-02fb-40d0-a13e-e77de60fe2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: faiss-cpu in /home/mlcore/conda/lib/python3.10/site-packages (1.13.2)\n",
      "Collecting faiss-gpu-cu12\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/faiss-gpu-cu12/1.13.2/faiss_gpu_cu12-1.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.4 MB)\n",
      "Collecting faiss-gpu-cu11\n",
      "  Using cached https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/faiss-gpu-cu11/1.13.2/faiss_gpu_cu11-1.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.4 MB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/mlcore/conda/lib/python3.10/site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in /home/mlcore/conda/lib/python3.10/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /home/mlcore/conda/lib/python3.10/site-packages (from faiss-gpu-cu12) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /home/mlcore/conda/lib/python3.10/site-packages (from faiss-gpu-cu12) (12.8.4.1)\n",
      "Collecting nvidia-cuda-runtime-cu11>=11.8.89 (from faiss-gpu-cu11)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cuda-runtime-cu11/11.8.89/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (875 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11>=11.11.3.6 (from faiss-gpu-cu11)\n",
      "  Downloading https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/packages/packages/nvidia-cublas-cu11/11.11.3.6/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, faiss-gpu-cu12, faiss-gpu-cu11\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/4\u001b[0m [faiss-gpu-cu11]m [faiss-gpu-cu11]u11]\n",
      "\u001b[1A\u001b[2KSuccessfully installed faiss-gpu-cu11-1.13.2 faiss-gpu-cu12-1.13.2 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-runtime-cu11-11.8.89\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu faiss-gpu-cu12 faiss-gpu-cu11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72622f4a-2dff-42e0-aeb1-4b85612f73a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.tcsbank.ru/artifactory/api/pypi/python-all/simple\n",
      "Requirement already satisfied: transformers in /home/mlcore/conda/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /home/mlcore/conda/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: sentence-transformers in /home/mlcore/conda/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: accelerate in /home/mlcore/conda/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: vllm in /home/mlcore/conda/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: filelock in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mlcore/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mlcore/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mlcore/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/mlcore/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/mlcore/.local/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/mlcore/conda/lib/python3.10/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: scikit-learn in /home/mlcore/conda/lib/python3.10/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /home/mlcore/conda/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: psutil in /home/mlcore/.local/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: cachetools in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (6.2.4)\n",
      "Requirement already satisfied: sentencepiece in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.2.1)\n",
      "Requirement already satisfied: blake3 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (1.0.8)\n",
      "Requirement already satisfied: py-cpuinfo in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: protobuf in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (6.32.1)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.128.0)\n",
      "Requirement already satisfied: aiohttp in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (3.13.2)\n",
      "Requirement already satisfied: openai>=1.99.1 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (2.15.0)\n",
      "Requirement already satisfied: pydantic>=2.12.0 in /home/mlcore/.local/lib/python3.10/site-packages (from vllm) (2.12.5)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /home/mlcore/.local/lib/python3.10/site-packages (from vllm) (0.23.1)\n",
      "Requirement already satisfied: pillow in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (11.3.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.12.0)\n",
      "Requirement already satisfied: lm-format-enforcer==0.11.3 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.11.3)\n",
      "Requirement already satisfied: llguidance<1.4.0,>=1.3.0 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (1.3.0)\n",
      "Requirement already satisfied: outlines_core==0.2.11 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.2.11)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (5.6.3)\n",
      "Requirement already satisfied: lark==1.2.2 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.27 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.1.27)\n",
      "Requirement already satisfied: partial-json-parser in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.2.1.1.post7)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /home/mlcore/.local/lib/python3.10/site-packages (from vllm) (27.1.0)\n",
      "Requirement already satisfied: msgspec in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: gguf>=0.17.0 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.17.1)\n",
      "Requirement already satisfied: mistral_common>=1.8.5 in /home/mlcore/conda/lib/python3.10/site-packages (from mistral_common[image]>=1.8.5->vllm) (1.8.8)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (4.11.0.86)\n",
      "Requirement already satisfied: einops in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.12.2 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.12.2)\n",
      "Requirement already satisfied: depyf==0.20.0 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: cloudpickle in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (3.1.2)\n",
      "Requirement already satisfied: watchfiles in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (1.1.1)\n",
      "Requirement already satisfied: python-json-logger in /home/mlcore/.local/lib/python3.10/site-packages (from vllm) (4.0.0)\n",
      "Requirement already satisfied: ninja in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (1.13.0)\n",
      "Requirement already satisfied: pybase64 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (1.4.3)\n",
      "Requirement already satisfied: cbor2 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (5.8.0)\n",
      "Requirement already satisfied: ijson in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (3.4.0.post0)\n",
      "Requirement already satisfied: setproctitle in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (1.3.7)\n",
      "Requirement already satisfied: openai-harmony>=0.0.3 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.0.8)\n",
      "Requirement already satisfied: anthropic==0.71.0 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.71.0)\n",
      "Requirement already satisfied: model-hosting-container-standards<1.0.0,>=0.1.9 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.1.13)\n",
      "Requirement already satisfied: mcp in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (1.25.0)\n",
      "Requirement already satisfied: numba==0.61.2 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.61.2)\n",
      "Requirement already satisfied: ray>=2.48.0 in /home/mlcore/conda/lib/python3.10/site-packages (from ray[cgraph]>=2.48.0->vllm) (2.53.0)\n",
      "Requirement already satisfied: torchaudio==2.9.0 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (2.9.0)\n",
      "Requirement already satisfied: torchvision==0.24.0 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.24.0)\n",
      "Requirement already satisfied: flashinfer-python==0.5.3 in /home/mlcore/conda/lib/python3.10/site-packages (from vllm) (0.5.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/mlcore/conda/lib/python3.10/site-packages (from anthropic==0.71.0->vllm) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/mlcore/conda/lib/python3.10/site-packages (from anthropic==0.71.0->vllm) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /home/mlcore/conda/lib/python3.10/site-packages (from anthropic==0.71.0->vllm) (0.17.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /home/mlcore/conda/lib/python3.10/site-packages (from anthropic==0.71.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/mlcore/conda/lib/python3.10/site-packages (from anthropic==0.71.0->vllm) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /home/mlcore/conda/lib/python3.10/site-packages (from anthropic==0.71.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: loguru in /home/mlcore/conda/lib/python3.10/site-packages (from compressed-tensors==0.12.2->vllm) (0.7.3)\n",
      "Requirement already satisfied: astor in /home/mlcore/conda/lib/python3.10/site-packages (from depyf==0.20.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: dill in /home/mlcore/.local/lib/python3.10/site-packages (from depyf==0.20.0->vllm) (0.4.0)\n",
      "Requirement already satisfied: apache-tvm-ffi<0.2,>=0.1 in /home/mlcore/conda/lib/python3.10/site-packages (from flashinfer-python==0.5.3->vllm) (0.1.7)\n",
      "Requirement already satisfied: click in /home/mlcore/conda/lib/python3.10/site-packages (from flashinfer-python==0.5.3->vllm) (8.3.0)\n",
      "Requirement already satisfied: nvidia-cudnn-frontend>=1.13.0 in /home/mlcore/conda/lib/python3.10/site-packages (from flashinfer-python==0.5.3->vllm) (1.17.0)\n",
      "Requirement already satisfied: nvidia-cutlass-dsl>=4.2.1 in /home/mlcore/conda/lib/python3.10/site-packages (from flashinfer-python==0.5.3->vllm) (4.3.5)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/mlcore/conda/lib/python3.10/site-packages (from flashinfer-python==0.5.3->vllm) (13.590.44)\n",
      "Requirement already satisfied: tabulate in /home/mlcore/conda/lib/python3.10/site-packages (from flashinfer-python==0.5.3->vllm) (0.9.0)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /home/mlcore/conda/lib/python3.10/site-packages (from lm-format-enforcer==0.11.3->vllm) (0.3.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/mlcore/conda/lib/python3.10/site-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/mlcore/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic==0.71.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/mlcore/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic==0.71.0->vllm) (3.10)\n",
      "Requirement already satisfied: certifi in /home/mlcore/conda/lib/python3.10/site-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/mlcore/conda/lib/python3.10/site-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/mlcore/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: jmespath in /home/mlcore/.local/lib/python3.10/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /home/mlcore/conda/lib/python3.10/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm) (80.9.0)\n",
      "Requirement already satisfied: starlette>=0.49.1 in /home/mlcore/conda/lib/python3.10/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm) (0.50.0)\n",
      "Requirement already satisfied: supervisor>=4.2.0 in /home/mlcore/conda/lib/python3.10/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm) (4.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/mlcore/.local/lib/python3.10/site-packages (from pydantic>=2.12.0->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/mlcore/.local/lib/python3.10/site-packages (from pydantic>=2.12.0->vllm) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/mlcore/.local/lib/python3.10/site-packages (from pydantic>=2.12.0->vllm) (0.4.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.0.4)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.21)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /home/mlcore/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.40.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.12.0)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.0.0 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.11.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /home/mlcore/conda/lib/python3.10/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.1)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.17.1)\n",
      "Requirement already satisfied: tomli>=2.0.0 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.9.0)\n",
      "Requirement already satisfied: rignore>=0.5.1 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.49.0)\n",
      "Requirement already satisfied: fastar>=0.8.0 in /home/mlcore/conda/lib/python3.10/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mlcore/.local/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /home/mlcore/conda/lib/python3.10/site-packages (from mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (4.25.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/mlcore/conda/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/mlcore/conda/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/mlcore/conda/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/mlcore/conda/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.27.1)\n",
      "Requirement already satisfied: cuda-python>=12.8 in /home/mlcore/conda/lib/python3.10/site-packages (from nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm) (13.1.1)\n",
      "Requirement already satisfied: cuda-bindings~=13.1.1 in /home/mlcore/conda/lib/python3.10/site-packages (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm) (13.1.1)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/mlcore/conda/lib/python3.10/site-packages (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm) (1.3.3)\n",
      "Requirement already satisfied: pycountry>=23 in /home/mlcore/conda/lib/python3.10/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (24.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/mlcore/conda/lib/python3.10/site-packages (from pydantic-settings>=2.0.0->fastapi[standard]>=0.115.0->vllm) (1.2.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/mlcore/conda/lib/python3.10/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.2)\n",
      "Requirement already satisfied: cupy-cuda12x in /home/mlcore/conda/lib/python3.10/site-packages (from ray[cgraph]>=2.48.0->vllm) (13.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mlcore/conda/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mlcore/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: rich>=13.7.1 in /home/mlcore/conda/lib/python3.10/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mlcore/conda/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mlcore/.local/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/mlcore/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mlcore/conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/mlcore/conda/lib/python3.10/site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/mlcore/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /home/mlcore/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.22.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/mlcore/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/mlcore/conda/lib/python3.10/site-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/mlcore/conda/lib/python3.10/site-packages (from aiohttp->vllm) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/mlcore/conda/lib/python3.10/site-packages (from aiohttp->vllm) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mlcore/conda/lib/python3.10/site-packages (from aiohttp->vllm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mlcore/conda/lib/python3.10/site-packages (from aiohttp->vllm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/mlcore/conda/lib/python3.10/site-packages (from aiohttp->vllm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/mlcore/conda/lib/python3.10/site-packages (from aiohttp->vllm) (1.22.0)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /home/mlcore/conda/lib/python3.10/site-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /home/mlcore/conda/lib/python3.10/site-packages (from mcp->vllm) (0.4.3)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /home/mlcore/conda/lib/python3.10/site-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (2.10.1)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /home/mlcore/conda/lib/python3.10/site-packages (from mcp->vllm) (3.1.2)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /home/mlcore/conda/lib/python3.10/site-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (46.0.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /home/mlcore/conda/lib/python3.10/site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/mlcore/conda/lib/python3.10/site-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (2.22)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/mlcore/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mlcore/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers torch sentence-transformers accelerate vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4290f08a-82d8-40b6-af73-e89d0386bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-large\" \n",
    "encoder = SentenceTransformer(MODEL_NAME, device=\"cuda\")  \n",
    "texts = df[\"message\"].tolist()\n",
    "doc_inputs = [\"passage: \" + t for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7436ed1c-6f09-413b-ae3b-03d76af07d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E_docs = encoder.encode(\n",
    "#     doc_inputs,\n",
    "#     batch_size=64,\n",
    "#     show_progress_bar=True,\n",
    "#     normalize_embeddings=True,\n",
    "# ).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb37ea91-457f-407e-bff0-7e743bf4430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "\n",
    "# dim = E_docs.shape[1]\n",
    "# index = faiss.IndexFlatIP(dim)      \n",
    "# index.add(E_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab301b0c-3979-48dd-97f9-c3c16f713b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import faiss\n",
    "# import pickle\n",
    "\n",
    "# OUT = Path(\"indexes\")\n",
    "# OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# #rowmap\n",
    "# rowmap = df[[\"message_id\",\"date\",\"date_day\",\"id_channel\",\"channel_name\"]].copy()\n",
    "# rowmap.to_parquet(OUT / \"rowmap.parquet\", index=False)\n",
    "\n",
    "# np.save(OUT / \"E_docs_e5_large.npy\", E_docs)\n",
    "\n",
    "# faiss.write_index(index, str(OUT / \"faiss_e5_large.index\"))\n",
    "\n",
    "# with open(OUT / \"bm25_corpus_tok.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(corpus_tok, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c4f45-33ca-49b9-b735-599e5d851055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af2760a8-4f82-4159-899f-e94447a94e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "INP = Path(\"indexes\")\n",
    "\n",
    "rowmap = pd.read_parquet(INP / \"rowmap.parquet\")\n",
    "\n",
    "E_docs = np.load(INP / \"E_docs_e5_large.npy\")\n",
    "index = faiss.read_index(str(INP / \"faiss_e5_large.index\"))\n",
    "\n",
    "with open(INP / \"bm25_corpus_tok.pkl\", \"rb\") as f:\n",
    "    corpus_tok = pickle.load(f)\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "bm25 = BM25Okapi(corpus_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcef3d8f-f6d5-4ded-b27d-b86e1b53b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(rowmap) == E_docs.shape[0]\n",
    "assert len(rowmap) == index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17969e2a-8bfc-414a-86a7-da15a18cfedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde85e14-f892-47a2-b585-4af1c1b8b5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f76acc5-fc79-4909-bfdd-c4211312fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def snippet(t: str, n: int = 1000) -> str:\n",
    "    return t[:n]\n",
    "\n",
    "def _topk_indices_from_scores(scores: np.ndarray, k: int) -> np.ndarray:\n",
    "    k = min(k, len(scores))\n",
    "    if k <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if k == len(scores):\n",
    "        idx = np.argsort(-scores)\n",
    "    else:\n",
    "        idx = np.argpartition(-scores, k-1)[:k]\n",
    "        idx = idx[np.argsort(-scores[idx])]\n",
    "    return idx.astype(int)\n",
    "\n",
    "def dense_candidates_faiss(index, encoder, query: str, topN: int = 500):\n",
    "    qv = encoder.encode([\"query: \" + query], normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
    "    scores, idx = index.search(qv, topN) \n",
    "    return idx[0].astype(int), scores[0].astype(np.float32)\n",
    "\n",
    "\n",
    "def hybrid_retrieve_rrf(\n",
    "    df: pd.DataFrame,\n",
    "    index,\n",
    "    encoder,\n",
    "    bm25,\n",
    "    tokenize_fn,\n",
    "    query: str,\n",
    "    k: int = 50,\n",
    "    topN_each: int = 500,\n",
    "    k_rrf: int = 60,\n",
    "    w_dense: float = 1.0,\n",
    "    w_bm25: float = 1.0,\n",
    "    exclude_message_id: str | None = None,\n",
    "    anchor_date: str | pd.Timestamp | None = None,\n",
    "    date_col: str = \"date_day\") -> pd.DataFrame:\n",
    "\n",
    "    if anchor_date is not None:\n",
    "        ad = pd.to_datetime(anchor_date, utc=True).normalize()\n",
    "\n",
    "        if date_col not in df.columns:\n",
    "            raise KeyError(f\"date_col='{date_col}' not found in df.columns\")\n",
    "\n",
    "        dts = pd.to_datetime(df[date_col], errors=\"coerce\", utc=True).dt.normalize()\n",
    "        allowed = (dts <= ad)\n",
    "        allowed_np = allowed.to_numpy(dtype=bool)\n",
    "    else:\n",
    "        allowed_np = None\n",
    "\n",
    "    d_idx, d_sc = dense_candidates_faiss(index, encoder, query, topN=topN_each)\n",
    "\n",
    "    if allowed_np is not None:\n",
    "        keep = allowed_np[d_idx]\n",
    "        d_idx = d_idx[keep]\n",
    "        d_sc = d_sc[keep]\n",
    "\n",
    "    dense_rank = {int(rowpos): r for r, rowpos in enumerate(d_idx, start=1)}\n",
    "\n",
    "    if bm25 is None:\n",
    "        out = df.iloc[d_idx].copy()\n",
    "        out[\"_rowpos\"] = d_idx\n",
    "        out[\"score_rrf\"] = (w_dense / (k_rrf + out[\"_rowpos\"].map(lambda rp: dense_rank.get(int(rp), np.nan)).astype(np.float32)))\n",
    "        out[\"rank_dense\"] = out[\"_rowpos\"].map(lambda rp: dense_rank.get(int(rp), np.nan))\n",
    "        out[\"rank_bm25\"] = np.nan\n",
    "\n",
    "        if exclude_message_id is not None and \"message_id\" in out.columns:\n",
    "            out = out[out[\"message_id\"].astype(str) != str(exclude_message_id)]\n",
    "\n",
    "        return out.head(k).reset_index(drop=True)\n",
    "\n",
    "    bm_scores = bm25.get_scores(tokenize_fn(query)).astype(np.float32)\n",
    "    if allowed_np is not None:\n",
    "        bm_scores[~allowed_np] = -np.inf\n",
    "\n",
    "    b_idx = _topk_indices_from_scores(bm_scores, topN_each)\n",
    "    bm_rank = {int(rowpos): r for r, rowpos in enumerate(b_idx, start=1)}\n",
    "\n",
    "    union = np.array(sorted(set(dense_rank) | set(bm_rank)), dtype=int)\n",
    "    rrf = np.zeros(len(union), dtype=np.float32)\n",
    "\n",
    "    for j, rowpos in enumerate(union):\n",
    "        if rowpos in dense_rank:\n",
    "            rrf[j] += w_dense / (k_rrf + dense_rank[rowpos])\n",
    "        if rowpos in bm_rank:\n",
    "            rrf[j] += w_bm25 / (k_rrf + bm_rank[rowpos])\n",
    "\n",
    "    order = np.argsort(-rrf)\n",
    "    union = union[order]\n",
    "    rrf = rrf[order]\n",
    "\n",
    "    out = df.iloc[union].copy()\n",
    "    out[\"_rowpos\"] = union\n",
    "    out[\"score_rrf\"] = rrf\n",
    "    out[\"rank_dense\"] = out[\"_rowpos\"].map(lambda rp: dense_rank.get(int(rp), np.nan))\n",
    "    out[\"rank_bm25\"] = out[\"_rowpos\"].map(lambda rp: bm_rank.get(int(rp), np.nan))\n",
    "\n",
    "    if exclude_message_id is not None and \"message_id\" in out.columns:\n",
    "        out = out[out[\"message_id\"].astype(str) != str(exclude_message_id)]\n",
    "\n",
    "    return out.head(k).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "149ac8fd-e146-4487-8cbb-3731554be0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-09-23 00:00:00+0000', tz='UTC'),\n",
       " Timestamp('2025-09-08 00:00:00+0000', tz='UTC'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"date_day\"].min(), df[\"date_day\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc92482-2a6e-4757-a3a7-6b9a96e0c03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b39a2-ec1c-4e6b-8b3d-8878c8c8a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20fd5266-3f63-4e49-ae0b-795767667a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# MODEL = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"cuda\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91710dd3-0196-4c7b-812c-b319832c1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "VRAM(GB): 79.3\n",
      "Model max_position_embeddings: 32768\n",
      "INFO 01-11 02:14:05 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 24576, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-32B-Instruct'}\n",
      "INFO 01-11 02:14:06 [model.py:514] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 01-11 02:14:06 [model.py:1661] Using max model len 24576\n",
      "INFO 01-11 02:14:06 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-11 02:14:08 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:14:16 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-32B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=24576, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-32B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m WARNING 01-11 02:14:16 [network_utils.py:36] The environment variable HOST_IP is deprecated and ignored, as it is often used by Docker and other software to interact with the container's network stack. Please use VLLM_HOST_IP instead to set the IP address for vLLM processes to communicate with each other.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:14:16 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.200.22.156:50141 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:14:16 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:14:17 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-32B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m /home/mlcore/conda/lib/python3.10/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:14:20 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:09,  1.66it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:01<00:09,  1.52it/s]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:01<00:09,  1.48it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:02<00:08,  1.46it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:03<00:08,  1.46it/s]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:04<00:07,  1.44it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:04<00:07,  1.37it/s]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:05<00:06,  1.48it/s]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:06<00:05,  1.48it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:06<00:04,  1.47it/s]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:07<00:04,  1.46it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:08<00:03,  1.42it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:08<00:02,  1.41it/s]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:09<00:02,  1.42it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:10<00:01,  1.41it/s]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:11<00:00,  1.41it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:11<00:00,  1.41it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:11<00:00,  1.44it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:14:34 [default_loader.py:308] Loading weights took 11.92 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:14:34 [gpu_model_runner.py:3659] Model loading took 61.0375 GiB memory and 16.437393 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:14:47 [backends.py:643] Using cache directory: /home/mlcore/.cache/vllm/torch_compile_cache/956b349265/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:14:47 [backends.py:703] Dynamo bytecode transform time: 12.25 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:15:07 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 13.861 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:15:07 [monitor.py:34] torch.compile takes 26.11 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:15:08 [gpu_worker.py:375] Available KV cache memory: 8.81 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:15:09 [kv_cache_utils.py:1291] GPU KV cache size: 36,064 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:15:09 [kv_cache_utils.py:1296] Maximum concurrency for 24,576 tokens per request: 1.47x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:06<00:00,  7.60it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:15:20 [gpu_model_runner.py:4587] Graph capturing finished in 11 secs, took 4.11 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=8132)\u001b[0;0m INFO 01-11 02:15:20 [core.py:259] init engine (profile, create kv cache, warmup model) took 45.68 seconds\n",
      "INFO 01-11 02:15:22 [llm.py:360] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "MODEL = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "#MODEL = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(MODEL, trust_remote_code=True)\n",
    "native_ctx = getattr(cfg, \"max_position_embeddings\", None)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"VRAM(GB):\", round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 1))\n",
    "print(\"Model max_position_embeddings:\", native_ctx)\n",
    "\n",
    "MAX_MODEL_LEN = 24576\n",
    "\n",
    "\n",
    "model = LLM(\n",
    "    model=MODEL,\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=0.90,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaab1396-2f2f-4691-80b6-4ddf925d8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Ğ¢Ñ‹ â€” Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº. Ğ¢Ñ‹ Ğ¿Ğ¸ÑˆĞµÑˆÑŒ Ğ°ĞºĞºÑƒÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚ Ğ¿Ğ¾ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑĞ¼ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ ĞºĞ¾Ñ€Ñ€ĞµÑĞ¿Ğ¾Ğ½Ğ´ĞµĞ½Ñ‚Ğ°.\n",
    "\n",
    "Ğ’Ñ…Ğ¾Ğ´: Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ° (YYYY-MM-DD) Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ¸Ğ´Ğ°:\n",
    "[id] date=YYYY-MM-DD channel(s)=<ĞºĞ°Ğ½Ğ°Ğ»1; ĞºĞ°Ğ½Ğ°Ğ»2; ...>\n",
    "<Ñ‚ĞµĞºÑÑ‚>\n",
    "Ğ’ÑĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ ĞĞ• ĞŸĞĞ—Ğ–Ğ• Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ°Ñ‚Ñ‹.\n",
    "\n",
    "ĞĞ‘Ğ©Ğ˜Ğ• ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ:\n",
    "1) ĞŸĞ¸ÑˆĞ¸ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².\n",
    "2) ĞĞµ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ¹ Ğ½Ğ¾Ğ¼ĞµÑ€Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ([id]).\n",
    "3) ĞĞµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ Ğ´Ğ°Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ â€œÑĞµĞ³Ğ¾Ğ´Ğ½Ñ/Ğ²Ñ‡ĞµÑ€Ğ°/Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾â€ â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ YYYY-MM-DD.\n",
    "4) Ğ›ÑĞ±Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ°/Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ/Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¸/Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ñ‹/Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ğ¾Ğ½Ğ¸ ĞµÑÑ‚ÑŒ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ….\n",
    "5) ĞšĞ°Ğ½Ğ°Ğ»Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ² Ğ´Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚Ğµ (Ğ² Ñ‚Ğ°Ğ¹Ğ¼Ğ»Ğ°Ğ¹Ğ½Ğµ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğµ Ğ±ÑƒĞ´ĞµÑ‚).\n",
    "6) Ğ•ÑĞ»Ğ¸ Ñ€ÑĞ´Ğ¾Ğ¼ Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ¾Ğ¹ Ğ¼Ğ°Ğ»Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â€” Ñ‡ĞµÑÑ‚Ğ½Ğ¾ ÑƒĞºĞ°Ğ¶Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ğ´Ğ°Ñ‚Ñƒ Ğ² Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ĞºĞµ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ², Ğ±ĞµĞ· Ğ´Ğ¾Ğ¼Ñ‹ÑĞ»Ğ¾Ğ² â€œÑ‡Ñ‚Ğ¾ ÑĞµĞ¹Ñ‡Ğ°Ñâ€.\n",
    "7) ĞĞµĞ»ÑŒĞ·Ñ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ â€œĞ¿ÑƒÑÑ‚Ñ‹Ğµâ€ Ğ´Ğ°Ñ‚Ñ‹ Ğ¸ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ²Ğ¸Ğ´Ğ° â€œĞ½ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…â€.\n",
    "\n",
    "ĞšĞĞš ĞŸĞ˜Ğ¡ĞĞ¢Ğ¬ Ğ”ĞĞ™Ğ”Ğ–Ğ•Ğ¡Ğ¢:\n",
    "- ĞĞ°Ñ‡Ğ½Ğ¸ Ñ 2â€“4 ÑĞ°Ğ¼Ñ‹Ñ… ÑĞ²ĞµĞ¶Ğ¸Ñ… Ğ£ĞĞ˜ĞšĞĞ›Ğ¬ĞĞ«Ğ¥ Ğ´Ğ°Ñ‚ Ğ² Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ĞºĞµ (ÑÑ‚Ğ¾ â€œĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñâ€).\n",
    "- Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ° ÑƒĞºĞ°Ğ¶Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº:\n",
    "  â€œĞšĞ°Ğ½Ğ°Ğ»(Ñ‹) (YYYY-MM-DD): â€¦â€\n",
    "- ĞĞµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ĞºĞ°Ğº Ğ¾Ğ´Ğ½Ğ¾ Ğ¸ Ñ‚Ğ¾ Ğ¶Ğµ: ÑĞ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ğ¹\n",
    "  â€œĞ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ / Ğ²Ğ½ĞµĞ±Ğ¸Ñ€Ğ¶ĞµĞ²Ğ¾Ğ¹ ĞºÑƒÑ€Ñ / Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ / Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° / Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·â€.\n",
    "- Ğ•ÑĞ»Ğ¸ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ñ‹ â€” Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞ¹.\n",
    "- ĞŸĞ¸ÑˆĞ¸ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸ Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾, ĞºĞ°Ğº Ğ² Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚Ğµ (Ğ±ĞµĞ· ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¼Ñ‹ÑĞ»Ğ¾Ğ²).\n",
    "- Ğ•ÑĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ ĞµÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°, Ñ‚Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµĞ³Ğ¾!\n",
    "\n",
    "Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ ĞĞ¢Ğ’Ğ•Ğ¢Ğ (3 Ğ±Ğ»Ğ¾ĞºĞ°):\n",
    "\n",
    "### 1) Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¸ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°\n",
    "* Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ: ...\n",
    "* ĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°: ...\n",
    "\n",
    "### 2) Ğ”Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚\n",
    "Ğ¡Ğ²ÑĞ·Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ (8â€“20 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğµ Ğ±Ğ¾Ğ»ĞµĞµ!), ĞºĞ°Ğº ĞºĞ¾Ñ€Ñ€ĞµÑĞ¿Ğ¾Ğ½Ğ´ĞµĞ½Ñ‚:\n",
    "- â€œĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñâ€: Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑĞ°Ğ¼Ñ‹Ğ¼ ÑĞ²ĞµĞ¶Ğ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°Ğ¼ + ĞºÑ‚Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ğ».\n",
    "- Ğ—Ğ°Ñ‚ĞµĞ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾ â€œÑ€Ğ°Ğ½ÑŒÑˆĞµ/Ğ¿Ñ€ĞµĞ´Ñ‹ÑÑ‚Ğ¾Ñ€Ğ¸Ñâ€: Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°Ğ¼ + ĞºÑ‚Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ğ».\n",
    "- 1â€“2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ: Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹, Ğ¸ÑÑ…Ğ¾Ğ´Ñ Ğ¸Ğ· Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ´Ğ°Ñ‚ÑŒ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ‚. ĞĞ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞ¼Ğ° Ğ² Ñ†ĞµĞ»Ğ¾Ğ¼ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ, ÑĞ²ĞµĞ¶Ğ°Ñ Ğ¸ Ğ²Ğ¸Ñ€ÑƒÑĞ½Ğ°Ñ.\n",
    "- Ğ˜ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, ĞµÑĞ»Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°, ĞºÑ€Ğ°Ñ‚ĞºĞ¾, Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚Ğ°!\n",
    "\n",
    "### 3) Ğ¢Ğ°Ğ¹Ğ¼Ğ»Ğ°Ğ¹Ğ½ (Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹, 1 Ğ´Ğ°Ñ‚Ğ° = 1 ÑÑ‚Ñ€Ğ¾ĞºĞ° = 1 Ñ„Ğ°ĞºÑ‚, Ğ±ĞµĞ· ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²)\n",
    "ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ’Ğ¡Ğ•Ğ¥ Ğ£ĞĞ˜ĞšĞĞ›Ğ¬ĞĞ«Ğ¥ Ğ´Ğ°Ñ‚ Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ğ¾Ñ‚ ÑĞ°Ğ¼Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ€Ğ¾Ğ¹ Ğº ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ (ascending).\n",
    "ĞĞ´Ğ½Ğ° ÑÑ‚Ñ€Ğ¾ĞºĞ° = Ğ¾Ğ´Ğ½Ğ° (1) Ğ´Ğ°Ñ‚Ğ°.\n",
    "ĞĞ´Ğ½Ğ° ÑÑ‚Ñ€Ğ¾ĞºĞ° = Ğ¾Ğ´Ğ¸Ğ½ (1) ÑĞ°Ğ¼Ñ‹Ğ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ñ„Ğ°ĞºÑ‚/Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾ ĞµÑÑ‚ÑŒ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… ÑÑ‚Ğ¾Ğ¹ Ğ´Ğ°Ñ‚Ñ‹.\n",
    "\n",
    "Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ ÑÑ‚Ñ€Ğ¾ĞºĞ¸:\n",
    "* YYYY-MM-DD â€” Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ğ»Ğ¸ / Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¾ÑÑŒ (1 Ñ„Ğ°ĞºÑ‚)\n",
    "\n",
    "ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ Ğ¢ĞĞ™ĞœĞ›ĞĞ™ĞĞ (ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾):\n",
    "- Ğ’ Ñ‚Ğ°Ğ¹Ğ¼Ğ»Ğ°Ğ¹Ğ½Ğµ ĞĞ• Ğ£ĞšĞĞ—Ğ«Ğ’ĞĞ™ ĞºĞ°Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ¾Ğ±Ñ‰Ğµ.\n",
    "- ĞĞµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ Ğ´Ğ°Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…!\n",
    "- ĞĞµ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ°Ñ‚Ğ°Ğ¼Ğ¸ â€œĞ¿Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ñƒâ€: Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ Ğ´Ğ°Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ´Ğ°Ñ‚Ñ‹!\n",
    "- ĞĞ¸ĞºĞ°ĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ â€œÑ‡Ñ‚Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ğ»Ğ¸â€!\n",
    "- Ğ•ÑĞ»Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ñƒ Ğ´Ğ°Ñ‚Ñƒ Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸ Ğ¾Ğ´Ğ¸Ğ½ ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾ÑÑ‚Ğ°Ğ²ÑŒ Ğ² Ğ´Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚Ğµ)!\n",
    "! Ğ•Ğ¡Ğ›Ğ˜ Ğ”ĞĞ¢Ğ« ĞĞ•Ğ¢ Ğ’ĞĞ£Ğ¢Ğ Ğ˜ ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢Ğ â€” Ğ•Ğ ĞĞ•Ğ›Ğ¬Ğ—Ğ¯ Ğ£ĞšĞĞ—Ğ«Ğ’ĞĞ¢Ğ¬. Ğ•Ğ¡Ğ›Ğ˜ Ğ¤ĞĞšĞ¢Ğ ĞĞ•Ğ¢ ĞĞ Ğ­Ğ¢Ğ£ Ğ”ĞĞ¢Ğ£ Ğ’ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢ĞĞ¥ â€” Ğ•Ğ“Ğ ĞĞ•Ğ›Ğ¬Ğ—Ğ¯ ĞŸĞ˜Ğ¡ĞĞ¢Ğ¬. !\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_rag_context(query: str, cand: pd.DataFrame, anchor_date: str, k_docs: int = 30, snip_chars: int = 850) -> str:\n",
    "    c = cand.head(k_docs).copy()\n",
    "\n",
    "    blocks = []\n",
    "    for i, row in enumerate(c.itertuples(index=False), start=1):\n",
    "        date_day = getattr(row, \"date_day\", getattr(row, \"date\", \"\"))\n",
    "        if isinstance(date_day, pd.Timestamp):\n",
    "            date_day = date_day.strftime(\"%Y-%m-%d\")\n",
    "        date_day = str(date_day)[:10]\n",
    "\n",
    "        channel = getattr(row, \"channel_name\")\n",
    "        text = getattr(row, \"message\", \"\")\n",
    "\n",
    "        blocks.append(f\"[{i}] date={date_day} channel={channel}\\n document=\" + snippet(str(text), snip_chars))\n",
    "\n",
    "    return (\n",
    "        f\"ĞĞšĞ¢Ğ£ĞĞ›Ğ¬ĞĞĞ¯ Ğ”ĞĞ¢Ğ ĞĞ‘Ğ—ĞĞ Ğ: {anchor_date}\\n\"\n",
    "        f\"Ğ’ĞĞŸĞ ĞĞ¡/Ğ—ĞĞŸĞ ĞĞ¡:\\n{query}\\n\\n\"\n",
    "        f\"Ğ˜Ğ¡Ğ¢ĞĞ§ĞĞ˜ĞšĞ˜:\\n\" + \"\\n\\n\".join(blocks)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b04c688-b520-422b-ab98-275c68d10ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# @torch.inference_mode()\n",
    "# def rag_summarize(sum_model, sum_tokenizer, query: str, cand: pd.DataFrame, anchor_date, \n",
    "#                   k_docs: int = 25, snip_chars: int = 900, max_new_tokens: int = 2000) -> str:\n",
    "    \n",
    "#     user = build_rag_context(query, cand, anchor_date=anchor_date, k_docs=k_docs, snip_chars=snip_chars)\n",
    "#     print(\"built context...\")\n",
    "    \n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "#         {\"role\": \"user\", \"content\": user},\n",
    "#     ]\n",
    "#     prompt = sum_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     enc = sum_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(sum_model.device)\n",
    "\n",
    "#     out_ids = sum_model.generate(\n",
    "#         **enc,\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         do_sample=False,\n",
    "#         eos_token_id=sum_tokenizer.eos_token_id,\n",
    "#         pad_token_id=sum_tokenizer.eos_token_id,\n",
    "#     )\n",
    "#     prompt_len = int(enc[\"attention_mask\"][0].sum().item())\n",
    "    \n",
    "#     return sum_tokenizer.decode(out_ids[0][prompt_len:], skip_special_tokens=True).strip(), user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79a6f6c0-27c0-48ec-b32f-94dcc09ce244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "def rag_summarize(sum_model, sum_tokenizer, query: str, cand: pd.DataFrame, anchor_date,\n",
    "                  k_docs: int = 25, snip_chars: int = 900, max_new_tokens: int = 2000):\n",
    "\n",
    "    user = build_rag_context(query, cand, anchor_date=anchor_date, k_docs=k_docs, snip_chars=snip_chars)\n",
    "    print(\"built context...\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "    prompt = sum_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    #Mistral\n",
    "    #prompt = f\"<s>[SYSTEM_PROMPT]{SYSTEM_PROMPT}[/SYSTEM_PROMPT][INST]{user}[/INST]\"\n",
    "\n",
    "    sampling = SamplingParams(\n",
    "        temperature=0.0,  \n",
    "        max_tokens=max_new_tokens,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "    result = sum_model.generate([prompt], sampling)[0]\n",
    "    text = result.outputs[0].text.strip()\n",
    "\n",
    "    return text, user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "382a7922-4c1b-4d3f-bbd7-95152e4842e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_hybrid(\n",
    "    df: pd.DataFrame,\n",
    "    index,\n",
    "    encoder,\n",
    "    bm25,\n",
    "    tokenize_fn,\n",
    "    query: str,\n",
    "    exclude_message_id: str | None = None,\n",
    "    k_retrieve: int = 50,\n",
    "    topN_each: int = 500,\n",
    "    k_docs: int = 25,\n",
    "    snip_chars: int = 1500,\n",
    "    max_new_tokens: int = 2000, \n",
    "    anchor_date: str = \"2025-09-04\", \n",
    "\n",
    "    sum_model=None,\n",
    "    sum_tokenizer=None):\n",
    "\n",
    "    \n",
    "    cand = hybrid_retrieve_rrf(\n",
    "        df=df,\n",
    "        index=index,\n",
    "        encoder=encoder,\n",
    "        bm25=bm25,\n",
    "        tokenize_fn=tokenize_fn,\n",
    "        query=query,\n",
    "        k=k_retrieve,\n",
    "        topN_each=topN_each,\n",
    "        k_rrf=60,\n",
    "        w_dense=1.0,\n",
    "        w_bm25=1.0,\n",
    "        exclude_message_id=exclude_message_id,\n",
    "        anchor_date=anchor_date)\n",
    "    \n",
    "    if sum_model is None or sum_tokenizer is None:\n",
    "        ctx = build_rag_context(query, cand, anchor_date=anchor_date, k_docs=k_docs, snip_chars=snip_chars)\n",
    "        return {\"context\": ctx, \"candidates\": cand, \"summary\": \"No LLM\"}\n",
    "    \n",
    "    summary, ctx = rag_summarize(sum_model, sum_tokenizer, query, cand, k_docs=k_docs, snip_chars=snip_chars, max_new_tokens=max_new_tokens, anchor_date=anchor_date)\n",
    "    return {\"context\": ctx, \"summary\": summary, \"candidates\": cand}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453b4b7-8475-4e75-87bf-3fd23950401c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4cdc31c-ad49-4c99-bab8-8d16d73c7f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built context...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0a1216bb184b5a92acb38d12c32377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b763dc08c7d64b0a8564dad65d02041f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = \"ĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ°\"\n",
    "\n",
    "out = run_rag_hybrid(\n",
    "    df=df,\n",
    "    index=index,\n",
    "    encoder=encoder,\n",
    "    bm25=bm25,\n",
    "    tokenize_fn=tokenize_ru,\n",
    "    query=q,\n",
    "    k_retrieve=30,\n",
    "    topN_each=2000,\n",
    "    k_docs=30,\n",
    "    snip_chars=1500,\n",
    "    max_new_tokens = 5000,\n",
    "    anchor_date = \"2025-09-04\",\n",
    "    sum_model=model,           \n",
    "    sum_tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8f8574b-ad55-46f5-b75f-9c8ef1738f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 1) Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¸ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°\n",
       "* Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ: ĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ°\n",
       "* ĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°: 2025-09-04\n",
       "\n",
       "### 2) Ğ”Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚\n",
       "ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ, Ğ½Ğ° 2025-07-28, Ğ¦Ğ‘ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ» ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ¸Ğ¶Ğµ 81 Ñ€ÑƒĞ±Ğ»Ñ, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ñ‚Ğ¾Ñ€Ğ° Ñ€ÑƒĞ±Ğ»Ñ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ğ½ĞµĞ±Ğ¸Ñ€Ğ¶ĞµĞ²Ğ¾Ğ³Ğ¾ ĞºÑƒÑ€ÑĞ°. ĞĞ° 2025-06-30, Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 97 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ Ğ¼Ğ°Ñ€Ñ‚Ğ° 2022 Ğ³Ğ¾Ğ´Ğ°. Ğ Ğ°Ğ½ĞµĞµ, Ğ½Ğ° 2025-05-29, Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ñ‚ Ğ¦Ğ‘ Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 79 Ñ€ÑƒĞ±Ğ»ĞµĞ¹, Ğ° Ğ²Ğ½ĞµĞ±Ğ¸Ñ€Ğ¶ĞµĞ²Ğ¾Ğ¹ ĞºÑƒÑ€Ñ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 79 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. ĞĞ° 2025-04-21, ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ´Ğ¾ 79 Ñ€ÑƒĞ±Ğ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¼Ğ°Ñ 2023 Ğ³Ğ¾Ğ´Ğ°. \n",
       "\n",
       "Ğ’ Ğ¿Ñ€ĞµĞ´Ñ‹ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ½Ğ° 2024-11-27, ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ğ» 114 Ñ€ÑƒĞ±Ğ»ĞµĞ¹, Ğ½Ğ¾ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ°Ñ‡Ğ°Ğ» ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒÑÑ. ĞĞ° 2024-12-06, Ğ¦Ğ‘ Ğ¿Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ğ» ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ¸Ğ¶Ğµ 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. \n",
       "\n",
       "Ğ¢ĞµĞ¼Ğ° ĞºÑƒÑ€ÑĞ° Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ğ²Ğ¸Ñ€ÑƒÑĞ½Ğ¾Ğ¹, Ğ¿Ğ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºÑƒÑ€ÑĞ° Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºÑƒ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ´Ğ½Ğ°ĞºĞ¾, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½Ğ¸Ğ¶Ğµ 80 Ñ€ÑƒĞ±Ğ»ĞµĞ¹.\n",
       "\n",
       "### 3) Ğ¢Ğ°Ğ¹Ğ¼Ğ»Ğ°Ğ¹Ğ½\n",
       "* 2024-11-14 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ²Ñ‹ÑˆĞµ 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2024-11-22 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ²Ñ‹ÑˆĞµ 102 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2024-11-27 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ 114 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2024-11-28 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 110 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2024-12-05 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ 103,3 Ñ€ÑƒĞ±Ğ»Ñ\n",
       "* 2024-12-06 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ¸Ğ¶Ğµ 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2024-12-24 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° ÑƒĞ¿Ğ°Ğ» Ğ½Ğ¸Ğ¶Ğµ 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2025-03-11 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ´Ğ¾ 86 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2025-03-17 â€” Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° ÑƒĞ¿Ğ°Ğ» Ğ½Ğ¸Ğ¶Ğµ 83 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2025-04-01 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ¸Ğ¶Ğµ 85 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2025-04-14 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¶ĞµĞ½ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 1 Ñ€ÑƒĞ±Ğ»ÑŒ\n",
       "* 2025-04-21 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ´Ğ¾ 79 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2025-05-21 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ 79,75 Ñ€ÑƒĞ±Ğ»Ñ\n",
       "* 2025-05-29 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 79 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\n",
       "* 2025-06-20 â€” Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ²Ñ‹ÑˆĞµ 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹ Ğ·Ğ° Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€\n",
       "* 2025-06-30 â€” Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 97 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ²\n",
       "* 2025-07-28 â€” ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ¸Ğ¶Ğµ 81 Ñ€ÑƒĞ±Ğ»Ñ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "import re\n",
    "\n",
    "def show_summary(summary: str):\n",
    "    if summary is None:\n",
    "        display(HTML(\"<b>summary is None</b>\"))\n",
    "        return\n",
    "\n",
    "    s = str(summary)\n",
    "    s = s.replace(\"\\\\n\", \"\\n\") \n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s).strip()\n",
    "\n",
    "    display(Markdown(s))\n",
    "\n",
    "show_summary(out[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e56b2f10-82de-46fe-be7d-a187c8d62254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ĞĞšĞ¢Ğ£ĞĞ›Ğ¬ĞĞĞ¯ Ğ”ĞĞ¢Ğ ĞĞ‘Ğ—ĞĞ Ğ: 2025-09-04\\nĞ’ĞĞŸĞ ĞĞ¡/Ğ—ĞĞŸĞ ĞĞ¡:\\nĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ°\\n\\nĞ˜Ğ¡Ğ¢ĞĞ§ĞĞ˜ĞšĞ˜:\\n[1] date=2024-11-27 channel=Ğ‘Ğ»ÑƒĞ¼Ğ±ĞµÑ€Ğ³\\n document=ĞšÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ°Ğ»Ğ¸Ğ» Ğ·Ğ° 111 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. UPD: ĞšÑƒÑ€Ñ ÑƒĞ¶Ğµ 114. ğŸ”µ Bloomberg\\n\\n[2] date=2025-07-28 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=Ğ¡Ğ²ĞµĞ¶Ğ¸Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ñ‚ Ğ¦Ğ‘ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ğ½ĞµĞ±Ğ¸Ñ€Ğ¶ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ñ‚Ğ¾Ñ€Ğ° Ñ€ÑƒĞ±Ğ»Ñ. @bankrollo\\n\\n[3] date=2024-12-06 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=Ğ¦Ğ‘ Ğ¿Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ğ» ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ¸Ğ¶Ğµ 100, ĞµĞ²Ñ€Ğ¾ â€” Ğ½Ğ¸Ğ¶Ğµ 107. @bankrollo\\n\\n[4] date=2025-04-21 channel=Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ Ğ¦Ğ‘\\n document=Ğ¦Ğ‘ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ» ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ¸Ğ¶Ğµ 81 Ñ€ÑƒĞ±Ğ»Ñ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ Ğ»ĞµÑ‚Ğ° 2023 Ğ³Ğ¾Ğ´Ğ° Ğ¦Ğ‘ Ğ Ğ¤ Ñ 22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ ÑĞ½Ğ¸Ğ·Ğ¸Ğ» Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¡Ğ¨Ğ Ğ½Ğ° 37.74 ĞºĞ¾Ğ¿., Ğ´Ğ¾ 80.7597 Ñ€ÑƒĞ±.\\n\\n[5] date=2024-11-14 channel=Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ° \\n document=ĞšÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ñ‚ĞµĞ¿ĞµÑ€ÑŒ ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ²Ñ‹ÑˆĞµ 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞœĞ¾ÑĞ±Ğ¸Ñ€Ğ¶Ğ¸. Â«Ğ¡Ñ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° â€” 100,2428 Ñ€ÑƒĞ±Ğ»ÑÂ», â€” ÑĞ¾Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€. Ğ¢Ğ°ĞºĞ¾Ğ³Ğ¾ ÑĞºĞ°Ñ‡ĞºĞ° Ğ½Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑÑÑ†ĞµĞ². Ğ¡ĞµĞ¹Ñ‡Ğ°Ñ Ğ¦ĞµĞ½Ñ‚Ñ€Ğ¾Ğ±Ğ°Ğ½Ğº ÑĞ°Ğ¼ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ†ĞµĞ½Ñƒ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¿Ğ¾ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğº Ñ€ÑƒĞ±Ğ»Ñ. Ğ­Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾ ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸ĞµĞ¼ ÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¹ Ğ¡Ğ¨Ğ. ğ”¼â„‚ğ•†â„•ğ•†ğ•„ğ•€ğ•‚ğ”¸\\n\\n[6] date=2025-05-29 channel=Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸\\n document=ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ñ‚ Ğ¦Ğ‘ Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 79 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. Ğ•Ğ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ³ - 78,5 Ñ€ÑƒĞ±Ğ»Ñ.\\n\\n[7] date=2025-06-30 channel=Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸\\n document=Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ°, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºÑƒÑ€Ñ Ğº Ğ²Ğ°Ğ»ÑÑ‚Ğ°Ğ¼ ÑˆĞµÑÑ‚Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½ - Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€Ğ¾Ğ² Ğ¡Ğ¨Ğ, ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ Ğ¾Ğ¿ÑƒÑĞºĞ°Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 97 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ Ğ¼Ğ°Ñ€Ñ‚Ğ° 2022-Ğ³Ğ¾ ĞŸĞ¾Ğ´Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ Ğ˜Ğ ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ / Ğ’ÑĞµ Ğ½Ğ°ÑˆĞ¸ ĞºĞ°Ğ½Ğ°Ğ»Ñ‹\\n\\n[8] date=2025-03-17 channel=Ğ Ğ°Ğ½ÑŒÑˆĞµ Ğ²ÑĞµÑ…. ĞÑƒ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸\\n document=Ğ Ğ°ÑÑ‡ĞµÑ‚Ğ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° - Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ±Ğ¸Ñ€Ğ¶ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ€Ñ‹Ğ½ĞºĞ° - ÑƒĞ¿Ğ°Ğ» Ğ½Ğ¸Ğ¶Ğµ 83 Ñ€ÑƒĞ±Ğ»ĞµĞ¹, Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾ Ğ»ĞµÑ‚Ğ°, ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ². Ğš 17.02 Ğ¼ÑĞº ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½Ğ° 2,52 Ñ€ÑƒĞ±Ğ»Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ - Ğ´Ğ¾ 82,87 Ñ€ÑƒĞ±Ğ»Ñ, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ° Ñ 27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ° Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ´Ğ°.\\n\\n[9] date=2024-04-19 channel=Ğ‘Ğ»ÑƒĞ¼Ğ±ĞµÑ€Ğ³\\n document=ĞšÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚ÑŒ 150-160 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. ĞĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ÑÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğº Ğ¸ÑĞ½Ñ Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ñ‚ 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 120 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. ĞŸĞ¾ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ±ÑƒĞ´ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¾Ğ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ¸. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğº ĞºĞ¾Ğ½Ñ†Ñƒ Ğ³Ğ¾Ğ´Ğ° Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ 150-160 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. ğŸ’¸ BloomEconomy\\n\\n[10] date=2025-04-14 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=Ğ¦ĞµĞ½Ñ‚Ñ€Ğ¾Ğ±Ğ°Ğ½Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ğ» ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 1 Ñ€ÑƒĞ±Ğ»ÑŒ, ĞµĞ²Ñ€Ğ¾ â€” Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 2. @bankrollo\\n\\n[11] date=2024-04-30 channel=Ğ‘Ğ»ÑƒĞ¼Ğ±ĞµÑ€Ğ³\\n document=Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑƒÑ€ÑĞ° ĞšÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° ÑƒĞ¿Ğ°Ğ» Ğ½Ğ¸Ğ¶Ğµ â‚½93. ĞŸÑ€ĞµĞ²Ñ‹ÑĞ¸Ğ² Ğ½Ğ°ĞºĞ°Ğ½ÑƒĞ½Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‚ĞºÑƒ â‚½93, Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°ĞºÑ€ĞµĞ¿Ğ¸Ñ‚ÑŒÑÑ Ğ²Ñ‹ÑˆĞµ Ğ½ĞµĞµ. Ğ•Ğ²Ñ€Ğ¾ Ñ‚Ğ¾Ñ€Ğ³ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ â‚½99,8, Ñ…Ğ¾Ñ‚Ñ ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ Ğ² Ñ€Ğ°Ğ¹Ğ¾Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒĞ´Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ¸Ğ²Ğ°Ğ» ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ â‚½100. ğŸ”µ BloomEconomy\\n\\n[12] date=2025-03-11 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=Ğ¦Ğ‘ Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ» ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ´Ğ¾ 86 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. Ğ•Ğ²Ñ€Ğ¾ ÑƒĞ¿Ğ°Ğ» Ğ´Ğ¾ 93,6 Ñ€ÑƒĞ±Ğ»Ñ. @bankrollo\\n\\n[13] date=2024-06-13 channel=Ğ‘Ğ»ÑƒĞ¼Ğ±ĞµÑ€Ğ³\\n document=ĞšÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° ÑƒĞ¿Ğ°Ğ» Ğ´Ğ¾ 87â‚½. ğŸ”µ Bloomberg\\n\\n[14] date=2025-06-19 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ¹Ğ¾Ğ½Ğµ 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹ Ğ·Ğ° Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€, Ğ·Ğ°ÑĞ²Ğ¸Ğ» Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²Ğ¸Ñ†Ğµ-Ğ¿Ñ€ĞµĞ¼ÑŒĞµÑ€ ĞœĞ°Ğ½Ñ‚ÑƒÑ€Ğ¾Ğ². Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ Ğ¿Ñ€Ğ¸ ĞºÑ€ĞµĞ¿ĞºĞ¾Ğ¼ Ñ€ÑƒĞ±Ğ»Ğµ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ¼Ğ°Ğ»Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼, Ğ¾Ñ‚Ğ¼ĞµÑ‚Ğ¸Ğ» Ğ¾Ğ½. @bankrollo\\n\\n[15] date=2024-12-24 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° ÑƒĞ¿Ğ°Ğ» Ğ½Ğ¸Ğ¶Ğµ 100. @bankrollo\\n\\n[16] date=2024-12-05 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=Ğ¦Ğ‘ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ» Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºÑƒÑ€ÑÑ‹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¸ ĞµĞ²Ñ€Ğ¾ â€” 103,3 Ğ¸ 109,7 ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. @bankrollo\\n\\n[17] date=2024-11-19 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ğ» 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. @bankrollo\\n\\n[18] date=2025-03-18 channel=Ğ Ğ°Ğ½ÑŒÑˆĞµ Ğ²ÑĞµÑ…. ĞÑƒ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸\\n document=Ğ’Ğ½ĞµĞ±Ğ¸Ñ€Ğ¶ĞµĞ²Ğ¾Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 81 Ñ€ÑƒĞ±Ğ»Ñ\\n\\n[19] date=2025-06-30 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ñ€ÑƒÑ…Ğ½ÑƒĞ» Ğ½Ğ¸Ğ¶Ğµ 97 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ Ğ¼Ğ°Ñ€Ñ‚Ğ° 2022 Ğ³Ğ¾Ğ´Ğ°. ĞĞ½ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ°Ğ»ÑÑ‚Ñ‹ Ğº ĞµĞ²Ñ€Ğ¾, Ğ¸ĞµĞ½Ğµ, ÑˆĞ²ĞµĞ¹Ñ†Ğ°Ñ€ÑĞºĞ¾Ğ¼Ñƒ Ñ„Ñ€Ğ°Ğ½ĞºÑƒ, Ğ±Ñ€Ğ¸Ñ‚Ğ°Ğ½ÑĞºĞ¾Ğ¼Ñƒ Ñ„ÑƒĞ½Ñ‚Ñƒ, ĞºĞ°Ğ½Ğ°Ğ´ÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ñƒ Ğ¸ ÑˆĞ²ĞµĞ´ÑĞºĞ¾Ğ¹ ĞºÑ€Ğ¾Ğ½Ğµ. @bankrollo\\n\\n[20] date=2024-04-26 channel=Ğ‘Ğ»ÑƒĞ¼Ğ±ĞµÑ€Ğ³\\n document=ĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºÑƒÑ€ÑÑ‹ Ğ²Ğ°Ğ»ÑÑ‚: ğŸ‡¸ Ğ”Ğ¾Ğ»Ğ»Ğ°Ñ€ â€“ 87.87â‚½ ğŸ‡ª Ğ•Ğ²Ñ€Ğ¾ â€“ 94.1â‚½ ğŸ‡¹ Ğ›Ğ¸Ñ€Ğ° â€“ 2.66â‚½ ğŸ‡µğŸ‡± Ğ—Ğ»Ğ¾Ñ‚Ñ‹Ğ¹ â€“ 21.91â‚½ ĞšÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ²Ğ°Ğ»ÑÑ‚Ğ°: Bitcoin â€“ 61172.00$ Ethereum â€“ 3371.35$ USDT â€“ 1.00$ Ton â€“ 7.60$ BloomEconomy\\n\\n[21] date=2025-04-01 channel=Ğ‘Ğ»ÑƒĞ¼Ğ±ĞµÑ€Ğ³\\n document=Ğ¦Ğ‘ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ» ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ¸Ğ¶Ğµ 85 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. ğŸ”µ Bloomberg\\n\\n[22] date=2025-06-20 channel=Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸\\n document=Ğ Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼ ÑĞµĞ¹Ñ‡Ğ°Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²Ğ°Ğ»ÑÑ‚Ğ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ²Ñ‹ÑˆĞµ 100 Ñ€ÑƒĞ±Ğ»ĞµĞ¹ Ğ·Ğ° Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€, Ğ·Ğ°ÑĞ²Ğ¸Ğ» Ğ³Ğ»Ğ°Ğ²Ğ° Ğ¡Ğ±ĞµÑ€Ğ±Ğ°Ğ½ĞºĞ° Ğ“ĞµÑ€Ğ¼Ğ°Ğ½ Ğ“Ñ€ĞµÑ„. ĞĞ½ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ½Ñ‹Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ â€“ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¹ â€“ ĞºÑƒÑ€Ñ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ²ÑĞµ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ğ°ÑĞ»Ğ¸ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¾Ğ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚. ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ° Ğ¿ÑÑ‚Ğ½Ğ¸Ñ†Ñƒ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 78,5 Ñ€ÑƒĞ±Ğ»Ñ.\\n\\n[23] date=2024-11-27 channel=Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ° \\n document=ĞœĞ¸Ğ½Ğ¸ÑÑ‚Ñ€Ñ‹ Ğ¸ Ğ´ĞµĞ¿ÑƒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºÑƒÑ€Ñ Ñ€ÑƒĞ±Ğ»Ñ. Ğ•Ğ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‡Ğ¸Ğ½Ğ¾Ğ²Ğ½Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ»ÑÑ Ğ¾ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸ â€” Ğ³Ğ»Ğ°Ğ²Ğ° ĞœĞ¸Ğ½Ñ„Ğ¸Ğ½Ğ° ĞĞ½Ñ‚Ğ¾Ğ½ Ğ¡Ğ¸Ğ»ÑƒĞ°Ğ½Ğ¾Ğ². ĞĞ½ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ» Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ ĞºÑƒÑ€Ñ Ñ€ÑƒĞ±Ğ»Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚ĞµÑ€Ğ¾Ğ². Ğ¡ĞµĞ¹Ñ‡Ğ°Ñ Ñ†ĞµĞ½Ğ° Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ° Ğ±Ğ¸Ñ€Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ 111 Ñ€ÑƒĞ±Ğ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑ‚Ğ¸, Ğ° ĞºÑƒÑ€Ñ ĞµĞ²Ñ€Ğ¾ Ğ²Ğ·Ğ»ĞµÑ‚ĞµĞ» Ğ´Ğ¾ 117 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. Ğ¡ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½ĞµĞ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€ Ğ²Ñ‹Ñ€Ğ¾Ñ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğ° 5,4%, Ğ° Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ¾ÑĞ±Ñ€Ñ â€” Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 12,5%. ğŸ¤‘ The Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ÑÑ‚\\n\\n[24] date=2025-05-29 channel=Ğ Ğ¸Ğ° ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸\\n document=Ğ’Ğ½ĞµĞ±Ğ¸Ñ€Ğ¶ĞµĞ²Ğ¾Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ 79 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\\n\\n[25] date=2025-05-21 channel=Forbes Russia\\n document=Ğ‘Ğ°Ğ½Ğº Ğ Ğ¾ÑÑĞ¸Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ» Ğ½Ğ° 22 Ğ¼Ğ°Ñ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ 79,75 Ñ€ÑƒĞ±Ğ»Ñ. ĞĞ¸Ğ¶Ğµ 80 Ñ€ÑƒĞ±Ğ»ĞµĞ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ Ğ¼Ğ°Ñ 2023 Ğ³Ğ¾Ğ´Ğ°. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞµĞ²Ñ€Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ°Ğ» Ğ´Ğ¾ 91,29 Ñ€ÑƒĞ±Ğ»Ñ, Ğ° ÑĞ°Ğ½ÑŒ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»ÑÑ\\n\\n[26] date=2024-11-22 channel=Forbes Russia\\n document=Ğ¦Ğ‘ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ» Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ²Ñ‹ÑˆĞµ 102 Ñ€ÑƒĞ±Ğ»ĞµĞ¹ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ñ Ğ¼Ğ°Ñ€Ñ‚Ğ° 2022 Ğ³Ğ¾Ğ´Ğ°. ĞĞ° 23 Ğ½Ğ¾ÑĞ±Ñ€Ñ ĞºÑƒÑ€Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ 102,58 Ñ€ÑƒĞ±Ğ»Ñ. Ğ¡Ğ°Ğ¼Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ±Ñ‹Ğ» Ğ·Ğ°Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ 11 Ğ¼Ğ°Ñ€Ñ‚Ğ° 2022-Ğ³Ğ¾, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ´ĞµĞ½ÑŒ Ğ¾Ğ½ Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ğ» 120 Ñ€ÑƒĞ±Ğ»ĞµĞ¹\\n\\n[27] date=2024-11-27 channel=Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ Ğ¦Ğ‘\\n document=Ğ”Ğ¾Ğ»Ğ»Ğ°Ñ€\\n\\n[28] date=2025-04-21 channel=Ğ‘Ğ»ÑƒĞ¼Ğ±ĞµÑ€Ğ³\\n document=ĞšÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ´Ğ¾ 79 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. ğŸ”µ Bloomberg\\n\\n[29] date=2025-02-25 channel=Ğ‘Ğ°Ğ½ĞºĞ¸, Ğ´ĞµĞ½ÑŒĞ³Ğ¸, Ğ´Ğ²Ğ° Ğ¾Ñ„ÑˆĞ¾Ñ€Ğ°\\n document=Ğ¦Ğ‘ Ğ¿Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ğ» ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ´Ğ¾ 86 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. @bankrollo\\n\\n[30] date=2024-11-28 channel=Ğ‘Ğ»ÑƒĞ¼Ğ±ĞµÑ€Ğ³\\n document=ĞšÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ° Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ½Ğ¸Ğ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‚ĞºĞ¸ 110 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. ğŸ”µ Bloomberg'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e25602-c922-4041-abc4-2fd0f98f6c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c19deabb-38f1-46f7-817d-ad259a0a74b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "VRAM (GB): 79.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "props = torch.cuda.get_device_properties(0)\n",
    "vram_gb = props.total_memory / (1024**3)\n",
    "print(\"GPU:\", props.name)\n",
    "print(\"VRAM (GB):\", round(vram_gb, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef50f0-d15a-4e45-a5da-2f723be7dc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bae4e7d-5f88-4561-afcb-e55ab536c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topics_available': 168, 'news_available': 32, 'topics_used': 168, 'news_used': 32, 'all_used': 200}\n",
      "{'row_id': 0, 'anchor_date': '2025-09-04', 'query_type': 'topic', 'query': 'ĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ°', 'seed_message_id': None}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_queries_from_parquet(parquet_path: str):\n",
    "    ev = pd.read_parquet(parquet_path)\n",
    "\n",
    "    if \"query\" not in ev.columns or \"query_type\" not in ev.columns:\n",
    "        raise KeyError(\"parquet must contain columns: query, query_type\")\n",
    "\n",
    "    items = []\n",
    "    for r in ev.itertuples(index=False):\n",
    "        q = getattr(r, \"query\", None)\n",
    "        qt = getattr(r, \"query_type\", None)\n",
    "        if q is None or qt is None:\n",
    "            continue\n",
    "        q = str(q).strip()\n",
    "        qt = str(qt).strip()\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        items.append({\n",
    "            \"row_id\": getattr(r, \"row_id\", None),\n",
    "            \"anchor_date\": str(getattr(r, \"anchor_date\", \"\")) if hasattr(r, \"anchor_date\") else \"\",\n",
    "            \"query_type\": qt,\n",
    "            \"query\": q,\n",
    "            \"seed_message_id\": getattr(r, \"seed_message_id\", None),\n",
    "        })\n",
    "\n",
    "    counts = {\n",
    "        \"topics_available\": int((ev[\"query_type\"].astype(str) == \"topic\").sum()),\n",
    "        \"news_available\": int((ev[\"query_type\"].astype(str) == \"news\").sum()),\n",
    "        \"topics_used\": sum(1 for x in items if x[\"query_type\"] == \"topic\"),\n",
    "        \"news_used\": sum(1 for x in items if x[\"query_type\"] == \"news\"),\n",
    "        \"all_used\": len(items),\n",
    "    }\n",
    "\n",
    "    return items, counts, ev\n",
    "\n",
    "items, counts, data = load_queries_from_parquet(\"eval_df_temporalrag.parquet\")\n",
    "print(counts)\n",
    "print(items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc7bb82f-7f53-42f3-b16c-05f05dc42352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_hybrid_batch(\n",
    "    df: pd.DataFrame,\n",
    "    index,\n",
    "    encoder,\n",
    "    bm25,\n",
    "    tokenize_fn,\n",
    "    sum_model,\n",
    "    sum_tokenizer,\n",
    "    items: list[dict],\n",
    "    k_retrieve: int = 50,\n",
    "    topN_each: int = 500,\n",
    "    k_docs: int = 25,\n",
    "    snip_chars: int = 1500,\n",
    "    max_new_tokens: int = 2000,\n",
    "    anchor_date_default: str = \"2025-09-04\",\n",
    "    gen_batch_size: int = 8,\n",
    "):\n",
    "    rows = []\n",
    "    prompts = []\n",
    "    prompt_row_ids = []\n",
    "\n",
    "    for it in items:\n",
    "        row_id = it.get(\"row_id\", None)\n",
    "        query = str(it.get(\"query\", \"\")).strip()\n",
    "        if not query:\n",
    "            continue\n",
    "\n",
    "        anchor_date = str(it.get(\"anchor_date\", \"\")).strip() or anchor_date_default\n",
    "        seed_message_id = it.get(\"seed_message_id\", None)\n",
    "        exclude_message_id = None if seed_message_id is None else str(seed_message_id)\n",
    "\n",
    "        cand = hybrid_retrieve_rrf(\n",
    "            df=df,\n",
    "            index=index,\n",
    "            encoder=encoder,\n",
    "            bm25=bm25,\n",
    "            tokenize_fn=tokenize_fn,\n",
    "            query=query,\n",
    "            k=k_retrieve,\n",
    "            topN_each=topN_each,\n",
    "            k_rrf=60,\n",
    "            w_dense=1.0,\n",
    "            w_bm25=1.0,\n",
    "            exclude_message_id=exclude_message_id,\n",
    "            anchor_date=anchor_date,\n",
    "        )\n",
    "\n",
    "        ctx = build_rag_context(\n",
    "            query=query,\n",
    "            cand=cand,\n",
    "            anchor_date=anchor_date,\n",
    "            k_docs=min(k_docs, len(cand)) if cand is not None else 0,\n",
    "            snip_chars=snip_chars,\n",
    "        )\n",
    "\n",
    "        rid = row_id if row_id is not None else len(rows)\n",
    "\n",
    "        rows.append({\n",
    "            \"row_id\": rid,\n",
    "            \"anchor_date\": anchor_date,\n",
    "            \"query\": query,\n",
    "            \"query_type\": it.get(\"query_type\", None),\n",
    "            \"seed_message_id\": seed_message_id,\n",
    "            \"exclude_message_id\": exclude_message_id,\n",
    "            \"k_retrieve\": k_retrieve,\n",
    "            \"topN_each\": topN_each,\n",
    "            \"k_docs\": k_docs,\n",
    "            \"snip_chars\": snip_chars,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"context\": ctx,\n",
    "            \"summary\": None,\n",
    "            \"n_cand\": int(len(cand)) if cand is not None else 0,\n",
    "        })\n",
    "\n",
    "        if sum_model is not None and sum_tokenizer is not None and cand is not None and len(cand) > 0:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": ctx},\n",
    "            ]\n",
    "            prompt = sum_tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "            prompt_row_ids.append(rid)\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "\n",
    "    if len(prompts) == 0:\n",
    "        return out_df\n",
    "\n",
    "    sampling = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    summaries = []\n",
    "    for start in range(0, len(prompts), int(gen_batch_size)):\n",
    "        chunk = prompts[start:start + int(gen_batch_size)]\n",
    "        outs = sum_model.generate(chunk, sampling)\n",
    "        for o in outs:\n",
    "            summaries.append(o.outputs[0].text.strip())\n",
    "\n",
    "    rid_to_summary = {rid: s for rid, s in zip(prompt_row_ids, summaries)}\n",
    "    out_df[\"summary\"] = out_df[\"row_id\"].map(rid_to_summary)\n",
    "\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b67d1d99-c8a5-4c41-8e12-70ccef824d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cf0b40fd8e4ee59d6645b53ef68218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bb1a313b32488087a1c3814d189e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f63222efe1d4cc98f15e50c109c948a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c51931a714649a8ad142e3c18447875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224a6d02d293490782b738b400993ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962f0e5b34414f918a8877c9d9bcfdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c5c9ac8ca443b8ab50a792926ce13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7947667dc20f48588dac35b25c852d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce9defb26404229831a70931371ec7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c49bf16ded4d028cf650abf9d7b7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7f17e54dd1428b882265cca882e6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e7b489536c40da8af575107c76fb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fef7b13d094f8cbe03b4532721fb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cef50a8797348e8ae6be79b2790747a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51a8500bb2940eb9a81c9d49156c904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22f596679ff44308146d39f84b532c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afda006bf7f499a8b0aaa6a65e94154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74eb98601b9e4f48ae9f546e77fbc878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6e2d5138df4ba39e384c06c44422b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced99d848e9843dc88bcbe332c2147e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db5185bbb9348fdbaddd5f1d9d8759a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2a8a324d4d4e5fa0733f849540f8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb3b96a7b6c4f9fb7cfdb8159b6d11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca4b853d0ae47c291e42bc01f5db0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edf4b50aaea427faca8ab7c4f1bfecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283f0c7690b2449494b3ec1e47c818e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b48be59a0e14cb3a7831a02300d818c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7609314f881c4ead9674b2e12b751739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed513a35cc142088ce764e8e52ea983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5751c4eaef1e418e94ddb12e15c89325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f90898e8e214c7fbd1901970fec68b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfe30ef9346417b97ab6852cb3eda20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0591aa7b0fa42f29acda457b214836e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca4d11de9ff4e6eacaf58f87f70711d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a408df9771d84ab7bfed866f069839e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6505605487d24a058c132be737c9940c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6688f864bd494fa28ec78e3db975b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d80a24ad1c4666941c6e22b859a657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393a4a1d45de46c7a9bad0b70d66d243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd8803b96214f7a9ddb220383f12451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d1a5611d544e52b136eac4ea44e1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bce5393e8d4f758fea11bb45a1d34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff114dfae6364515bb10734dc9338bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3570bfc61864176a6e95a94e5d0a550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de445d456f7641229a60a53b832d4990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e33e909862b4183bf73b95047d5f8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194bafe2a3ec4650a338ce1168c9a496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8ce9d7f4db4fa5b79efe1de2453e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434637babd5541e08d070b62cc7a3dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79fe3275ce1492ca34198e0c5e5e72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df_rag = run_rag_hybrid_batch(\n",
    "    df=df,\n",
    "    index=index,\n",
    "    encoder=encoder,\n",
    "    bm25=bm25,\n",
    "    tokenize_fn=tokenize_ru,\n",
    "    sum_model=model,\n",
    "    sum_tokenizer=tokenizer,\n",
    "    items=items,\n",
    "    k_retrieve=150,\n",
    "    topN_each=1000,\n",
    "    k_docs=30,\n",
    "    snip_chars=800,\n",
    "    max_new_tokens=2000,\n",
    "    gen_batch_size=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f57ebafb-e7ca-423b-949b-86bad48b8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_rag.to_parquet(\"eval_df_rag_default.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58069a8-6326-4db4-a4ad-d93f352638de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db71e0-e65e-4243-bbc9-40a57ee5b077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa86a20-14cf-48d4-89e4-80f66537337e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e1578-a774-4a1f-ba07-c3e2db786e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6a85c-9116-434c-84d9-1bf8fc18ebdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34296b50-956e-4686-bf34-1e18d2893733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "local/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
